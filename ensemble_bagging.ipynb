{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Bagging Ensembles\n",
    "\n",
    "- bagged decision tree\n",
    "- random forest\n",
    "- exremely randomized tree\n",
    "\n",
    "Bagging is a bootstrap ensemble method that creates individuals for its ensemble by training each classifier on a random redistribution of the training set. Each classifier's training set is generated by randomly drawing, with replacement, N examples - where N is the size of the original training set; many of the original examples may be repeated in the resulting training set while others may be left out. Each individual classifier in the ensemble is generated with a different random sampling of the training set.\n",
    "\n",
    "\n",
    "## Decision Tree (high variance) and Bagged Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = pd.DataFrame(iris.data[:, :], columns = iris.feature_names[:])\n",
    "y = pd.DataFrame(iris.target, columns =[\"Species\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting Dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 20, random_state = 100)\n",
    "\n",
    "# Defining the stump\n",
    "stump = DecisionTreeClassifier(max_depth = 1)\n",
    "\n",
    "# Creating an ensemble \n",
    "ensemble = BaggingClassifier(base_estimator = stump, n_estimators = 1000,bootstrap = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training classifiers\n",
    "stump.fit(X_train, np.ravel(y_train))\n",
    "ensemble.fit(X_train, np.ravel(y_train))\n",
    "\n",
    "# Making predictions\n",
    "y_pred_stump = stump.predict(X_test)\n",
    "y_pred_ensemble = ensemble.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the stump is 55.0 %\n",
      "The accuracy of the ensemble is 55.0 %\n"
     ]
    }
   ],
   "source": [
    "# Determine performance\n",
    "stump_accuracy = metrics.accuracy_score(y_test, y_pred_stump)\n",
    "ensemble_accuracy = metrics.accuracy_score(y_test, y_pred_ensemble)\n",
    "\n",
    "# Print message to user\n",
    "print(f\"The accuracy of the stump is {stump_accuracy*100:.1f} %\")\n",
    "print(f\"The accuracy of the ensemble is {ensemble_accuracy*100:.1f} %\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created 1000 decision stumps that were exactly the same. It’s like we asked a single person what their favorite food was 1000 times and, not surprisingly, obtained the same answer 1000 times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier (medium variance)\n",
    "\n",
    "Random forest models reduce the risk of overfitting by introducing randomness by:\n",
    "- building multiple trees (n_estimators)\n",
    "- drawing observations with replacement (i.e., a bootstrapped sample)\n",
    "- splitting nodes on the best split among a random subset of the features selected at every node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the stump\n",
    "stump = DecisionTreeClassifier(max_depth = 1, splitter = \"best\", max_features = \"sqrt\")\n",
    "\n",
    "# Create Random Forest \n",
    "ensemble = BaggingClassifier(base_estimator=stump,n_estimators=1000,bootstrap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training classifiers\n",
    "stump.fit(X_train, np.ravel(y_train))\n",
    "ensemble.fit(X_train, np.ravel(y_train))\n",
    "\n",
    "# Making predictions\n",
    "y_pred_tree = stump.predict(X_test)\n",
    "y_pred_ensemble = ensemble.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the stump is 55.0 %\n",
      "The accuracy of the Random Forest is 90.0 %\n"
     ]
    }
   ],
   "source": [
    "# Determine performance\n",
    "stump_accuracy = metrics.accuracy_score(y_test, y_pred_stump)\n",
    "ensemble_accuracy = metrics.accuracy_score(y_test, y_pred_ensemble)\n",
    "\n",
    "# Print message to user\n",
    "print(f\"The accuracy of the stump is {stump_accuracy*100:.1f} %\")\n",
    "print(f\"The accuracy of the Random Forest is {ensemble_accuracy*100:.1f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So by simply introducing variation, we were able to obtain an accuracy of 95 %. In other words, decision stumps with low accuracies were used to build a forest. Variation was introduced among the stumps by building them on bootstraps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Tree Classifier (low variance)\n",
    "\n",
    "Extremely Randomized Trees Classifier(Extra Trees Classifier) is a type of bagged decision tree ensemble method. The random trees are constructed from the samples of the training dataset. \n",
    "\n",
    "Extra Trees is like RF, in that it builds multiple trees and splits nodes using random subsets of features, but with two key differences: it does not bootstrap observations (meaning it samples without replacement), and nodes are split on random splits, not best splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the stump\n",
    "stump = DecisionTreeClassifier(max_depth = 1, splitter = \"random\", max_features = \"sqrt\")\n",
    "\n",
    "# Create Extra Trees\n",
    "ensemble = BaggingClassifier(base_estimator=stump,n_estimators=1000,bootstrap=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training classifiers\n",
    "stump.fit(X_train, np.ravel(y_train))\n",
    "ensemble.fit(X_train, np.ravel(y_train))\n",
    "\n",
    "# Making predictions\n",
    "y_pred_tree = stump.predict(X_test)\n",
    "y_pred_ensemble = ensemble.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stump Accuracy 55.0 %\n",
      "ExtraTree Accuracy 95.0 %\n"
     ]
    }
   ],
   "source": [
    "# Determine performance\n",
    "stump_accuracy = metrics.accuracy_score(y_test, y_pred_stump)\n",
    "ensemble_accuracy = metrics.accuracy_score(y_test, y_pred_ensemble)\n",
    "\n",
    "# Print message to user\n",
    "print(f\"Stump Accuracy {stump_accuracy*100:.1f} %\")\n",
    "print(f\"ExtraTree Accuracy {ensemble_accuracy*100:.1f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Trees show high variance, Random Forests show medium variance, and Extra Trees show low variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest (RF optimizes splits on trees)\n",
    "\n",
    "RF is a bagged Decision Tree (precisely Decision stumps) Ensemble method. The RF algorihm is essentially the combination of two independent ideas: bagging, and random selection of features.\n",
    "\n",
    "DTs work well when they are of small depth. Higher depth DTs are more prone to overfitting and thus lead to higher variance in the model. This shortcoming of DT is explored by the Random Forest model. The Purpose of Random Forest is to improve Prediction Accuracy.\n",
    "\n",
    "\n",
    "### Parameter tunings\n",
    "\n",
    "As you increase max_depth you increase variance and decrease bias. On the other hand, as you increase min_samples_leaf you decrease variance and increase bias. So, these parameters will control the level of regularization when growing the trees. In summary, decreasing any of the max* parameters and increasing any of the min* parameters will increase regularization.\n",
    "\n",
    "- a) max_depth: The maximum depth that you allow the tree to grow to. The deeper you allow, the more complex your model will become. Increasing max_depth, training error will always go down (or at least not go up).The deeper the tree, the more splits it has and it captures more information about the data.\n",
    "- b) min_samples_split: min_sample_split is the minimum no. of samples or data points required for a split. For instance, if min_sample_split = 6 and there are 4 sample in the node, then the split will not happen (regardless of entropy).\n",
    "- c) min_sample_leaf: the minimum number of data points required in each leaf to continue the splits.This parameter is similar to min_samples_splits, however, this describe the minimum number of samples of samples at the leafs, the base of the tree.\n",
    "- d) max_leaf_nodes: the maximum number of (supposedly best) leaf nodes.\n",
    "\n",
    "### OUT OF BAG Error\n",
    "\n",
    "RF technique involves sampling of the input data with replacement (bootstrap sampling). In this sampling, about one thrird of the data is not used for training and can be used to testing.These are called the out of bag samples. Error estimated on these out of bag samples is the out of bag error.\n",
    "The OOB error is often used for assessing the prediction performance of RF. An advantage of the OOB error is that the complete original sample is used both for constructing the RF classifier and for error estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the difference between Bagging and Random Forest?\n",
    "\n",
    "bagging is not same as random forest.\n",
    "\n",
    "Bagging creates randomized samples of the data set and grows trees on a different sample of the original data. The remaining 1/3 of the sample is used to estimate unbiased OOB error. It considers all the features at a node (for splitting). Once the trees are fully grown, it uses averaging or voting to combine the resultant predictions.\n",
    "\n",
    "The need for random forest surfaced after discovering that the bagging algorithm results in correlated trees when faced with a dataset having strong predictors. Unfortunately, averaging several highly correlated trees doesn't lead to a large reduction in variance.\n",
    "\n",
    "But how do correlated trees emerge? Good question! Let's say a data set has a very strong predictor, along with other moderately strong predictors. In bagging, a tree grown every time would consider the very strong predictor at its root node, thereby resulting in trees similar to each other.\n",
    "\n",
    "The main difference between random forest and bagging is that random forest considers only a subset of predictors at a split. This results in trees with different predictors at top split, thereby resulting in decorrelated trees and more reliable average output. That's why we say random forest is robust to correlated predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision stump , a decision tree classifier with depth set to one (two splits)\n",
    "\n",
    "# DT vs RF\n",
    "\n",
    "The first is the number of features to consider when looking for the best split at each tree node: while DT considers all the features, RF considers a random subset of them, of size equal to the parameter max_features.\n",
    "\n",
    "The second is that, while DT considers the whole training set, a single RF tree considers only a bootstrapped sub-sample of it.\n",
    "\n",
    "Averaging ensembles such as a RandomForestClassifier and ExtraTreesClassifier are meant to tackle the variance problems (lack of robustness with respect to small changes in the training set) of individual DecisionTreeClassifier instances.\n",
    "\n",
    "#### to make single decisin tree using RF\n",
    "\n",
    "```python\n",
    "clf = RandomForestClassifier(n_estimators=1, max_features=None, bootstrap=False)\n",
    "\n",
    "```\n",
    "\n",
    "in which case neither bootstrap sampling nor random feature selection will take place, and the performance should be roughly equal to that of a single decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RF vs ET\n",
    "\n",
    "ERT do not resample observations when building a tree. (They do not perform bagging.)\n",
    "\n",
    "ERT do not use the “best split.”\n",
    "Like a RF, ERT select a random subset of predictors for each split. \n",
    "Instead of the “best split” for the predictors, ERT makes a small number of randomly chosen splits-points for each of the selected predictors. In the original method, this value was 1. (A tuning parameter: numRandomCuts)\n",
    "ERT then selects the “best split” from this small number of choices.\n",
    "\n",
    "Random forest that develop each decision tree from a bootstrap sample of the training dataset, the Extra Trees algorithm fits each decision tree on the whole training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
