{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVMs are a set of supervised learning methods used for classification, regression and outliers detection. Support vectors are the data points that lie closest to the decision surface (or hyperplane)\n",
    "\n",
    "- effective in cases where number of dimensions is greater than the number of samples.\n",
    "\n",
    "- SVMs decision function depends on some subset of the training data, called the support vectors\n",
    "\n",
    "- Support Vector Machine algorithms are not scale invariant, so it is highly recommended to scale your data.\n",
    "\n",
    "- high performance with little tuning\n",
    "\n",
    "- hyperplance separates the datapoints, but real data cannont be separated , there C defines amount of violation of the marging allowed\n",
    "\n",
    "- SVM is robust to multi-collinearity and overfitting\n",
    "\n",
    "- usage Text and Hypertext Categorization, Clustering of new articles\n",
    "\n",
    "- Because they are affected only by points near the margin, they work well with high-dimensional data—even data with more dimensions than samples, which is a challenging regime for other algorithms.\n",
    "- disadvantage : SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold crossvalidation\n",
    "\n",
    "- It is useful for both linearly Separable (hard margin) and Non-linearly Separable (soft margin) data.\n",
    "\n",
    "- Non-Linear data is basically which cannot be separated with a straight line\n",
    "\n",
    "When training an SVM with the Radial Basis Function (RBF) kernel, two parameters must be considered: C and gamma. The parameter C, common to all SVM kernels, trades off misclassification of training examples against simplicity of the decision surface. A low C makes the decision surface smooth, while a high C aims at classifying all training examples correctly. gamma defines how much influence a single training example has. The larger gamma is,\n",
    "the closer other examples must be to be affected.\n",
    "\n",
    "\n",
    "In Machine Learning, we have to keep variance and bias well-balanced to have a good fit model. If you decrease the margin with SVM classifiers, the bias will decrease and the variance will increase (and vice versa, where an increase in the margin will result in an increase in bias and decrease in variance). Notice that adding more instances outside of the margins or “off the street” will not affect the decision boundary at all (the line between the margin).\n",
    "\n",
    "__Performance of SVM Models__\n",
    "\n",
    "Cost and Gamma are the hyper-parameters that decide the performance of an SVM model. Model selection in this class of SVMs involves two hyperparameters: the penalty parameter C and the kernel width σ.\n",
    "\n",
    "__gamma parameter__\n",
    "\n",
    "- The gamma parameter makes most intuitive sense when we think about the RBF (or Gaussian) kernel.\n",
    "- gamma is a parameter of the RBF kernel (gaussian kernel) to handle non-linear classification and can be thought of as the ‘spread’ of the kernel and therefore the decision region. \n",
    "- The larger the gamma, the narrow the gaussian \"bell\" is\n",
    "-  It is the kernel coefficient for the ‘rbf’, ‘poly’ and ‘sigmoid’.\n",
    "\n",
    "- When gamma is low, the ‘curve’ of the decision boundary is very low and thus the decision region is very broad. When gamma is high, the ‘curve’ of the decision boundary is high, which creates islands of decision-boundaries around data points. \n",
    "\n",
    "- higher the gamma values tries to exactly fit the training data (only nearby points are considered)\n",
    "- small gamma - less complexity\n",
    "- higher gamma - higher complexity  --> generalization error and cause overfitting issues\n",
    "\n",
    "\n",
    "a High value of Gamma leads to more accuracy but biased results and vice-versa. Similarly, a large value of Cost parameter (C) indicates poor accuracy but low bias and vice-versa.\n",
    "\n",
    "- With a lower value of Gamma will create a loose fit of the training dataset. On the contrary, a high value of gamma will allow the model to get fit more appropriately. A low value of gamma only provides consideration to the nearby points for the calculation of a separate plane whereas the high value of gamma will consider all the data-points to calculate the final separation line.\n",
    "\n",
    "__Cost Parameter C__\n",
    "- C is the cost of misclassification (penalty parameter). it as the degree of correct classification that the algorithm has to meet \n",
    "-  This parameter is a tradeoff between maximizing the margin and minimizing the error.\n",
    "\n",
    "- C is (violations of margins). So higher C means more errors are allowed (which will lead to lower variance).\n",
    "\n",
    "- The hardness of the margin is controlled by a tuning parameter C, which is tuned using a cross validation.\n",
    "- C is a parameter of the SVC learner and is the penalty for misclassifying a data point. When C is small, the classifier is okay with misclassified data points (high bias, low variance). When C is large, the classifier is heavily penalized for misclassified data and therefore bends over backwards avoid any misclassified data points (low bias, high variance).\n",
    "\n",
    "- The cost parameter decides how much an SVM should be allowed to “bend” with the data. For a low cost, you aim for a smooth decision surface and for a higher cost, you aim to classify more points correctly. It is also simply referred to as the cost of misclassification.\n",
    "\n",
    "__C is a regularization parameter:__\n",
    "- small  → large margin (and Weight of misclassified points is increased)_\n",
    "- large C → narrow margin\n",
    "- C = ∞ hard margin\n",
    "\n",
    "Small C makes the cost of misclassificaiton low (\"soft margin\"). Large C makes the cost of misclassification high ('hard margin\"), thus forcing the algorithm to explain the input data stricter and potentially overfit.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q How to use SGD as linerSVM and LR?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# alpha is main pararmeter\n",
    "# Linear SVM loss='hinge'\n",
    "clf = SGDClassifier( class_weight='balanced', alpha=0.001, penalty='l2', loss='hinge', random_state=42) \n",
    "\n",
    "# LR when loss='log'\n",
    "clf = SGDClassifier( class_weight='balanced', alpha=0.001, penalty='l2', loss='log', random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C and alpha both have the same effect. The difference is a choice of terminology. C is proportional to 1/alpha. You should use GridSearchCV to select either alpha or C the same way, but remember a higher C is more likely to overfit, where a lower alpha is more likely to overfit.\n",
    "\n",
    "SGDClassifier uses stochastic gradient descent in which the data is fed through the learning algorithm sample by sample. The n_iter tells it how many passes it should make over the data. As the number of iterations goes up and the learning rate goes down, SGD becomes more like batch gradient descent, but it becomes slower as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q Why linear SVM works well for high dimensional problems?__\n",
    "\n",
    "\n",
    "Linear SVMs are designed to handle high-dimensional data, like in the case of text classification.  SVMs eliminate the need for feature selection\n",
    "\n",
    "- Preprocessing the features using PCA/LDA did not significantly increases classification accuracy of the SVM.\n",
    "\n",
    "\n",
    "In linear SVM only the cost of missclassification \"C\" is the hyper-parameter to be tuned.\n",
    "\n",
    "In non-linear SVM you also have to tune the kernel hyper-parameters.\n",
    "\n",
    "This makes SVM a very good tool for very high dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__What is kernerl Trick?__ \n",
    "\n",
    "\n",
    "The kernel function is a measure of similarity between two sets of features.\n",
    "\n",
    "Non-linear SVM is used when the data can’t be separated using a straight line. We use kernel functions in this case that help transform the data into another dimension that has a clear dividing margin between the two classes. Kernel functions help transform non-linear spaces into linear spaces.\n",
    "\n",
    "- kernel parameter selects types of hyperplance to separate data. eg. liner, rbf, poly\n",
    "- kernals = ['liner','rbf','poly','sigmoid']\n",
    "- rbf is also called gaussian kernel\n",
    "- Here “rbf” and “poly” are useful for non-linear hyper-plane.\n",
    "- degree parameter is used when kernel is set to 'poly'. Its the degree of polynominal.\n",
    "- degree == 1 means liner kernel\n",
    "-  Since linear kernel is a special case of rbf kernel. Linear kernels are rarely used in practice\n",
    "\n",
    "__Radial Basis Function__\n",
    "\n",
    "The radial basis part of the name comes from the fact that this function decreases in value as it moves away from the center (support vecotor)\n",
    "\n",
    "Recall a kernel expresses a measure of similarity between vectors. The RBF kernel represents this similarity as a decaying function of the distance between the vectors (i.e. the squared-norm of their distance). \n",
    "Thus, closer vectors have a larger RBF kernel value than farther vectors. This function is of the form of a bell-shaped curve.\n",
    "The γ parameter sets the width of the bell-shaped curve. The larger the value of γ the narrower will be the bell. Small values of γ yield wide bells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q When the C parameter is set to infinite, which of the following holds true?__\n",
    "\n",
    "At such a high level of misclassification penalty, soft margin will not hold existence as there will be no room for error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q Suppose you are using RBF kernel in SVM with high Gamma value. What does this signify?__\n",
    "\n",
    "- The model would consider only the points close to the hyperplane for modeling\n",
    "\n",
    "a RBF (radial basis function) kernel is used when the boundaries are hypothesized to be curve-shaped.\n",
    "\n",
    "RBF kernel uses two main parameters, gamma and C that are related to:\n",
    "\n",
    "the decision region (how spread the region is), and\n",
    "the penalty for misclassifying a data point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "__Q What is a hard margin?__\n",
    "\n",
    "A hard margin means that an SVM is very rigid in classification and tries to work extremely well in the training set, causing overfitting.\n",
    "\n",
    "There are two main issues with hard margin classification. \n",
    "- Hard Margin Classification only works if the data is linearly separable \n",
    "- Hard Margins are very sensitive to outliers. We can use soft margin classifications to avoid these issues. \n",
    "-  C = ∞ enforces all constraints hence hard margin\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q What is the loss function that linear SVM optimizes?__\n",
    "\n",
    "- Linear SVM is also called Large Margin Classifier.\n",
    "\n",
    "Like Logistic Regression, SVM’s cost function is convex as well. The most popular optimization algorithm for SVM is Sequential Minimal Optimization that can be implemented by ‘libsvm’ package in python. SMO solves a large quadratic programming(QP) problem by breaking them into a series of small QP problems that can be solved analytically to avoid time-consuming process to some degree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q What is Hinge Loss?__\n",
    "\n",
    "- The hinge loss is used for \"maximum-margin\" classification, most notably for support vector machines (SVMs). \n",
    "\n",
    "- Hinge loss is primarily used with Support Vector Machine (SVM) Classifiers with class labels -1 and 1.\n",
    "- The hinge loss is a convex function, so many of the usual convex optimizers used in machine learning can work with it\n",
    "\n",
    "- This is a loss function used for training classifiers.\n",
    "\n",
    "\n",
    "The hinge loss term ∑imax(0,1−yi(w⊺xi+b)) in soft margin SVM penalizes misclassifications. In hard margin SVM there are, by definition, no misclassifications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " __Q. What is the advantage of performing dimensionality reduction before fitting an SVM?__\n",
    "\n",
    "Support Vector Machine Learning Algorithm performs better in the reduced space. It is beneficial to \n",
    "perform dimensionality reduction before fitting an SVM if the number of features is large when \n",
    "compared to the number of observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Why SVM works well for text categorization.?__\n",
    "\n",
    "Svm perform good for text classification task because this algo automatically give weights to the feature and perform well on nonlinear data. Because of kernals it has.\n",
    "\n",
    "1. High Dimensional input space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q What is The fundamental difference between discriminative models and generative model?__\n",
    "\n",
    "- Discriminative models learn the (hard or soft) boundary between classes\n",
    "- Generative models model the distribution of individual classes\n",
    "\n",
    "\n",
    "SVMs and decision trees are discriminative because they learn explicit boundaries between classes. SVM is a maximal margin classifier, meaning that it learns a decision boundary that maximizes the distance between samples of the two classes, given a kernel. The distance between a sample and the learned decision boundary can be used to make the SVM a \"soft\" classifier. DTs learn the decision boundary by recursively partitioning the space in a manner that maximizes the information gain (or another criterion).\n",
    "\n",
    "When the aim of algorithm is only to classify, then discriminative approach may be better and less expensive than generative approach, as following the generative approach to model input distribution can result in requiring too much training data to model complexities in distribution which are unimportant for computing the posteriors required for decision making."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q Why did you use Random Forests instead of Clustering?__\n",
    "\n",
    "When choosing between decision trees and clustering, remember that decision trees are themselves a clustering method. The leaves of a decision tree contain clusters of records that are similar to one another and dissimilar from records in other leaves. The difference between the clusters found with a decision tree and the clusters found using other methods such as K-means, agglomerative algorithms, or self-organizing maps is that decision trees are directed while the other techniques I mentioned are undirected.\n",
    "\n",
    "Decision trees are appropriate when there is a target variable for which all records in a cluster should have a similar value. Records in a cluster will also be similar in other ways since they are all described by the same set of rules, but the target variable drives the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q. When would you use random forests Vs SVM and why?__\n",
    "\n",
    "SVM and Random Forest are both used in classification problems.\n",
    "\n",
    "- If you are sure that your data is outlier free and clean then go for SVM otherwise  Random forest.\n",
    "\n",
    "- In a case of a multi-class classification problem: SVM will require one-against-all method (memory intensive)\n",
    "\n",
    "- If one needs to know the variable importance (random forests can perform it as well)\n",
    "\n",
    "- If one needs to get a model fast (SVM is long to tune, need to choose the appropriate kernel and its parameters, for instance sigma and epsilon)\n",
    "\n",
    "- In a semi-supervised learning context (random forest and dissimilarity measure): SVM can work only in a supervised learning mode\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- Random Forest machine learning algorithms are preferred for multiclass problems.\n",
    "- SVM is preferred in multi-dimensional problem set - like text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q. Which kernels do you know? How to choose a kernel?__\n",
    "\n",
    "- Gaussian kernel\n",
    "- Linear kernel\n",
    "- Polynomial kernel\n",
    "- Laplace kernel\n",
    "- Esoteric kernels: string kernels, chi-square kernels\n",
    "\n",
    "Tex Classification\n",
    "- If number of features is large (relative to number of observations): SVM with linear kernel ; e.g. text classification with lots of words, small training example\n",
    "- If number of features is small, number of observations is intermediate: Gaussian kernel\n",
    "- If number of features is small, number of observations is small: linear kernel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q How to get probabilities for SGDClassifier (LinearSVM) ?__\n",
    "\n",
    "\n",
    "In the strict sense, that's not possible.\n",
    "\n",
    "Support vector machine classifiers are non-probabilistic: they use a hyperplane (a line in 2D, a plane in 3D and so on) to separate points into one of two classes. Points are only defined by which side of the hyperplane they are on., which forms the prediction directly.\n",
    "\n",
    "This is in contrast with probabilistic classifiers like logistic regression and decision trees, which generate a probability for every point that is then converted to a prediction.\n",
    "\n",
    "CalibratedClassifierCV is a sort of meta-estimator; to use it, you simply pass your instance of a base estimator to its constructor, so this will work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "clf = SGDClassifier(loss='hinge')\n",
    "calibrated_clf = CalibratedClassifierCV(clf, cv=5, method='sigmoid')\n",
    "#calibrated_clf.fit(X, y)\n",
    "\n",
    "# Note that this is equivalent to what sklearn.SVM.SVC does anyway.\n",
    "# eg. Calibration of Random forest can be used reduce log loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When performing classification you often want not only to predict the class label, but also obtain a probability of the respective label. This probability gives you some kind of confidence on the prediction. Some models can give you poor estimates of the class probabilities and some even do not not support probability prediction. The calibration module allows you to better calibrate the probabilities of a given model, or to add support for probability prediction.\n",
    "\n",
    "Well calibrated classifiers are probabilistic classifiers for which the output of the predict_proba method can be directly interpreted as a confidence level. For instance, a well calibrated (binary) classifier should classify the samples such that among the samples to which it gave a predict_proba value close to 0.8, approximately 80% actually belong to the positive class\n",
    "\n",
    "__Caliberation of ML Algorithms__\n",
    "- Platt Scaling (or Sigmoid Scaling)\n",
    "- Isotonic Regression (non parametric form of regression)\n",
    "\n",
    "__BRIER SCORE__\n",
    "\n",
    "- the Breier Score is a measure of caliberation.\n",
    "- it measures the accuracy of probabilistic predictions. \n",
    "- It is applicable to tasks in which predictions must assign probabilities to a set of mutually exclusive discrete outcomes\n",
    "- The best possible Brier score is 0, for total accuracy.\n",
    "- The lowest possible score is 1, which mean the forecast was wholly inaccurate.\n",
    "- Brier Score before caliberation and after caliberation is used.\n",
    "\n",
    "A brier score is a way to verify the accuracy of a probability forecast. A probability forecast refers to a specific event, such as there is a 25% probability of it raining in the next 24 hours. The score can only be used for binary outcomes, where there are only two possible events, like “it rained” or “it didn’t rain.” It could also be used for categorical outcomes as long as they can be structured as binary outcomes (i.e. “true” or “false”).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ROC__ \n",
    "\n",
    "- ROC Curves are used to see how well your classifier can separate positive and negative examples and to identify the best threshold for separating them.\n",
    "- TPR (sensitivity) vs FPR (1-specifity) curve\n",
    "- ROC shows trade off between sensitivity and specificity\n",
    "- ROC evaluates as binary classsifiers\n",
    "- It is a measure of rank, estimating the probability that a random positive is ranked before random negative, without committing to a particular decision threshold.\n",
    "\n",
    "- ideal ROC : shows a combination of two straight lines\n",
    "- Random (baseline) - A classifier with the random performance level always shows a straight line (eg. a coin flip, useless model)\n",
    "- Curves close to the perfect ROC curve have a better performance level than the ones closes to the baseline\n",
    "\n",
    "__AUROC__\n",
    "- the probability that a classifier will rank a randomly chosen positive instance higher than a randomly chosen negative example.\n",
    "- AUROC (1 for a perfect model) is used to get a measure of rank \n",
    "- AUC scores are convenient to compare multiple classifiers.\n",
    "- The AUROC is more informative than accuracy for imbalanced data\n",
    "- AUC score is between 0 and 1 (AUC for meaningful classifier > 0.5)\n",
    "\n",
    "__Selcting best threshold__\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "error = bias + variance\n",
    "__How to Achieve Low Error__\n",
    "variance reduction for a complex model, \n",
    "or bias reduction for a simple model,\n",
    "\n",
    "__Boosting__\n",
    "- Boosting is based on weak learners (high bias, low variance)\n",
    "- boosting reduces bias of a large number of \"small\" models with low variance.\n",
    "- because boosted trees are derived by optimizing an objective function, basically it can be used to solve almost all objective we can write gradient out. This including things like ranking, poission regression, which RF is harder to achieve.\n",
    "\n",
    "__Bagging__\n",
    "Random Forest is a bagging Algorithm than boosting one. Error term consist of bias and variance.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__RF vs GBT__\n",
    "\n",
    "__The two main differences are:\n",
    "\n",
    "- How trees are built: random forests builds each tree independently while gradient boosting builds one tree at a time, where each new tree helps to correct errors made by previously trained tree. ( and additive model)\n",
    "\n",
    "- Combining results: random forests combine results at the end of the process (by averaging or \"majority rules\") while gradient boosting combines results along the way.\n",
    "\n",
    "- Strength of the model: \n",
    "\n",
    "If you carefully tune parameters, gradient boosting can result in better performance than random forests. However, gradient boosting may not be a good choice if you have a lot of noise, as it can result in overfitting. They also tend to be harder to tune than random forests.\n",
    "\n",
    "The first step involves Bootstrapping technique for training and testing and second part involves decision tree for the prediction purpose. Now like every other predictive modelling technique we have the goal to minimize the generalization of error. To decrease that we make the balance between the bias and variance called as Bias-variance tradeoff.\n",
    "\n",
    " In a random forest, multiple trees are grown using a bagging technique (bootstrapped trees) in parallel and then the average of each tree prediction is taken in case we are dealing with the regression or a majority vote if classification. On the other hand, in boosting the trees are grown sequentially such that one tree is learning from the previous tree and by doing so we boost learning and increase the prediction accuracy.\n",
    "\n",
    "- the biggest difference between RFs and GBTs is how they optimize the bias–variance tradeoff in order to reduce generalization error. Both RFs and GBTs are ensembles of decision trees\n",
    "\n",
    "- RF uses decision trees, which are very prone to overfitting. In order to achieve higher accuracy, RF decides to create a large number of them based on bagging. The basic idea is to resample the data over and over and for each sample train a new classifier. Different classifiers overfit the data in a different way, and through voting those differences are averaged out.\n",
    "\n",
    "- GBM is a boosting method, which builds on weak classifiers. The idea is to add a classifier at a time, so that the next classifier is trained to improve the already trained ensemble. Notice that for RF each iteration the classifier is trained independently from the rest.\n",
    "\n",
    "- Gradient boosting uses regression trees for prediction purpose where a random forest use decision tree.\n",
    "\n",
    "- The boosting strategy for training takes care the minimization of bias which the random forest lacks.\n",
    "- The random forest is easy to parallelize but boosted trees are hard to do.\n",
    "- GBMs are more sensitive to overfitting if the data is noisy.\n",
    "- GBMs are harder to tune than RF. There are typically three parameters: number of trees, depth of trees and learning rate, and each tree built is generally shallow.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q Why do GBMs outperform Random Forests?__\n",
    "\n",
    "Because gradient boosting grow trees sequentially (longer training time) where each tree corrects the classification error of the previous tree which boost the overall performance, while random forest grow trees independently and the classification of each test instance is calculated using averaging/ majority voting.\n",
    "\n",
    "__Random Forests__ train each tree independently, using a random sample of the data. This randomness helps to make the model more robust than a single decision tree, and less likely to overfit on the training data. There are typically two parameters in RF - number of trees and no. of features to be selected at each node.\n",
    "\n",
    "__GBTs__ build trees one at a time, where each new tree helps to correct errors made by previously trained tree. With each tree added, the model becomes even more expressive. There are typically three parameters - number of trees, depth of trees and learning rate, and the each tree built is generally shallow.\n",
    "\n",
    "Although it may seem GBDTs are better than random forests, GBDTs are prone to overfitting, however there are strategies to overcome same and build more generalized trees using a combination of parameters like learning rate (shrinkage) and depth of tree.  Generally these two parameters are kept on the lower side to allow for slow learning and better generalization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q What is slack variable in SVM?__\n",
    "\n",
    "in SVMs, our optimization objective is to maximize the margin. The primary reason for having decision boundaries with large margins is that they tend to have a lower generalization error whereas models with small margins are more prone to overfitting \n",
    "\n",
    "The reason for introducing the slack variable ξ is that the linear constraints need to be relaxed for nonlinearly separable data to allow convergence of the optimization in the presence of misclassifications under the appropriate cost penalization.\n",
    "\n",
    "With the variable C, we can penalize for misclassification. Large values of C correspond to large error penalties while we are less strict about misclassification errors if we choose smaller values for C. We can then use the parameter C to control the width of the margin and therefore tune the bias-variance trade-off as shown in the picture below:\n",
    "\n",
    "The basic morale of kernel methods is to deal with linearly inseparable data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
