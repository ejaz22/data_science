{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "\n",
    "Algorithm\n",
    "\n",
    "    - Objective             : Max. IG at each split\n",
    "    - Loss Function         : Cross Entropy loss\n",
    "    - Optimization Function : Hyperparameter optimizations\n",
    "    - Output                : Actual labels or values\n",
    "\n",
    "Fundamental questions for tree construction are:\n",
    "\n",
    "    - Will tree be binary or not?\n",
    "    - Which attribute will be node?\n",
    "    - How missing data will be handled?\n",
    "    - If tree becomes too large,how to prune it?\n",
    "\n",
    "__Quick Terms__\n",
    "- CART (Classification and Regression Trees) → uses Gini Index as classification metric.\n",
    "- ID3 (Iterative Dichotomiser 3) → uses Entropy function and Information gain as metrics.\n",
    "- root represents whole training dataset\n",
    "- nodes represent features\n",
    "- branches represent possible values connecting feature\n",
    "- leaf represents a class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DT is a Supervised learning algorithm, logical model, non-parameteric. DTs are high variance algorithm, meaning that different splits in the training data can lead to very different trees. The main objective of the decision tree is to split data in such a way that each element in one group belongs to the same category. \n",
    "\n",
    "## Splitting Criteria\n",
    "\n",
    "    - Gini index (binary DTs)\n",
    "    - Information gain (binary DTs)\n",
    "    - chi_square (non-binary DTs)\n",
    "    \n",
    " \n",
    "## Gini Impurity or Gini Index\n",
    "\n",
    "The Gini index is a cost function used to evaluate splits in the dataset. It measures, how often a randomly chosen element would be incorrectly classified, therefore lower gini is prefrerred.\n",
    "\n",
    "The degree of the Gini index falls between 0 and 1, where 0 denotes that all the elements belong to a certain class and 1 denotes that the elements are randomly distributed across various classes. When the value of Gini is equal to 0, the node is considered pure and no further split is done. \n",
    "\n",
    "## Entropy\n",
    "\n",
    "Entropy is a measure of node impurity (homogeneouty). For a binary class (a,b), the formula to calculate it is shown below. Entropy is maximum at p = 0.5. For p(X=a)=0.5 or p(X=b)=0.5 means, a new observation has a 50%-50% chance of getting classified in either classes. The entropy is minimum when the probability is 0 or 1.\n",
    "\n",
    "Entropy measure homogeneouty therefore its max when class is uniformly distributed. The entropy is 0 if all samples at a node belong to the same class, and the Entropy is maximal if we have an uniform class distribution\n",
    "\n",
    "## Information Gain (IG)\n",
    "\n",
    "- information gain is an statistical property which ensure expected reduction in entropy after split\n",
    "- Information gain is used to determine which feature gives us the maximum information about a class. IG is derived from entropy. \n",
    "- High entropy means that we have a collection of different classes and a low entropy means that we have predominantly one class, therefore, we keen on splitting the node in a way that decreases the entropy. \n",
    "\n",
    "- Information gain is the decrease in entropy. IG computes the difference between entropy before split and average entropy after split of the dataset based on given attribute values. ID3 (Iterative Dichotomiser) decision tree algorithm uses information gain.\n",
    "\n",
    "- IG = information before splitting (parent) — information after splitting (children)\n",
    "\n",
    "- Information gain is biased for the attribute with many outcomes. It means it prefers the attribute with a large number of distinct values. For instance, consider an attribute with a unique identifier such as customer_ID has zero info(D) because of pure partition. This maximizes the information gain and creates useless partitioning.\n",
    "\n",
    "\n",
    "### Assumptions\n",
    "\n",
    "- In the beginning, the whole training set is considered at the root.\n",
    "- The root node (the first decision node) partitions the data using the feature that provides the most information gain.\n",
    "- Feature values are preferred to be categorical. If values are continuous then they are discretized prior to building the model.\n",
    "- Records are distributed recursively on the basis of attribute values.\n",
    "- Order to placing attributes as root or internal node of the tree is done by using some statistical approach.\n",
    "\n",
    "### How it works\n",
    "-  It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes.\n",
    "\n",
    "### Pruning\n",
    "- When we remove sub-nodes of a decision node, this process is called pruning. You can say opposite process of splitting.\n",
    "- To achieve a trade-of between prediction accuracy and complexity\n",
    "\n",
    "__How to choose and optimal max_depth for the tree?__\n",
    "\n",
    "- a pruned tree that is less complex, explainable, and easy to understand.\n",
    "\n",
    "- The core algorithm for building decision trees called ID3.  ID3 uses Entropy and Information Gain to construct a decision tree. ID3 algorithm uses entropy to calculate the homogeneity of a sample.\n",
    "\n",
    "- Not fit for continuous variables: While working with continuous numerical variables, decision tree looses information when it categorizes variables in different categories.\n",
    "\n",
    "- It requires fewer data preprocessing from the user, for example, there is no need to normalize columns.\n",
    "\n",
    "- The small variation(or variance) in data can result in the different decision tree. This can be reduced by bagging and boosting algorithms.\n",
    "\n",
    "- Decision trees are biased with imbalance dataset, so it is recommended that balance out the dataset before creating the decision tree.\n",
    "\n",
    "__Other Parameters__\n",
    "\n",
    "criterion: It is used to measure the quality of a split in the decision tree classification. By default, it is ‘gini’; it also supports ‘entropy’.\n",
    "\n",
    "max_depth: This is used to add maximum depth to the decision tree after the tree is expanded.\n",
    "\n",
    "min_samples_leaf: This parameter is used to add the minimum number of samples required to be present at a leaf node.\n",
    "\n",
    "## Visualizing decsion tree\n",
    "\n",
    "Using graphviz we can visualize the tree. export_graphviz function converts decision tree classifier into dot file and pydotplus convert this dot file to png or displayable form on Jupyter.\n",
    "The attribute to be made node is selected using:\n",
    "\n",
    "    - Information gain in ID3\n",
    "    - Gain ratio in C 4.5\n",
    "    - Ginni  CART\n",
    "\n",
    "While id3 and c4.5 both lead to multiway split, CART can have binary or multiway split based on choice of splitting criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gini Impurity vs Entropy \n",
    "\n",
    "There are three commonly used impurity measures used in binary decision trees. Most of the times, performance of a model won’t change whether you use Gini or Entropy.In terms of computation, Entropy takes more time as it includes Log function.\n",
    "\n",
    "\n",
    "- Entropy (a way to measure impurity)\n",
    "- Gini index (a criteria to minimize probability of misclassification)\n",
    "- and Classification Error\n",
    "- Both Gini Impurity and Entropy are criteria to split a node in a decision tree. \n",
    "- Gini is intended for continuous attributes, and Entropy for attributes that occur in classes.\n",
    "- Gini will tend to find the largest class, and entropy tends to find groups of classes that make up ~50% of the data.\n",
    "- Gini to minimize misclassification and Entropy for exploratory analysis.\n",
    "- Because the ensemble model(RF) is quite robust and resistant to noise from the individual decision trees, we typically don't need to prune the random forest, and the only parameter we care about is the number of trees i.e k\n",
    "\n",
    "In classification trees, the Gini Index is used to compute the impurity of a data partition. So Assume the data partition D consisiting of 4 classes each with equal probability. Then the Gini Index (Gini Impurity) will be: \n",
    "Gini(D) = 1 - (0.25^2 + 0.25^2 + 0.25^2 + 0.25^2)\n",
    "\n",
    "Similar to the Entropy, the Gini Impurity is maximal if the classes are perfectly mixed. However, in practice both Gini Impurity and Entropy typically yield very similar results and it is often not worth spending much time on evaluating trees using different impurity criteria rather than experimenting with different pruning cut-offs. Another impurity measure is the Classification Error\n",
    "\n",
    "__NOTE__ \n",
    "\n",
    "- If the sample is completely homogeneous the entropy is zero and if the sample is an equally divided it has entropy of one.\n",
    "- The entropy is 0 if all samples of a node belong to the same class, and the entropy is maximal if we have a uniform class distribution. In other words, the entropy of a node (consist of single class) is zero because the probability is 1 and log (1) = 0. Entropy reaches maximum value when all classes in the node have equal probability.\n",
    "\n",
    "- Gini index is an intermediate measure between entropy and the classification error.\n",
    "\n",
    "- underfitting (high bias)\n",
    "\n",
    "- In order to avoid overfitting, it is necessary to use additional techniques (e.g. cross-validation, regularization, early stopping, pruning, or Bayesian priors).\n",
    "\n",
    "- Regularization is a way of finding a good bias-variance tradeoff by tuning the complexity of the model. It is a very useful method to handle collinearity (high correlation among features), filter out noise from data, and eventually prevent overfitting. The concept behind regularization is to introduce additional information (bias) to penalize extreme parameter weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When would you use Decision tree over Random Forest?\n",
    "\n",
    "- When the data is small and  is more non-parametric in nature and we are not worried about accuracy on future datasets.\n",
    "- Easy to compute and explain why a particular variable is having higher importance\n",
    "- If the goal is exploratory analysis, we should prefer a single DT , as to understand the data relationship in a tree hierarchy structure.\n",
    "- The tree can be visualized and hence, for non-technical users, it is easier to explain model implementation\n",
    "- DTs are prone to overfitting. Setting a max depth solves overfitting but introduces bias.\n",
    "\n",
    "Random forest should be preferred if:\n",
    "- If the goal is better predictions, we should prefer RF, to reduce the variance. when accuracy is prioritised over explainability\n",
    "- when data has high bias, employing bagging and sampling techniques correctly will reduce over fitting\n",
    "\n",
    "If we need to analyse a bigger dataset, with a lot of features with a higher demand for accuracy a Random Forest would be a better choice.\n",
    "By bootstraping over our dataset, a Random Forest would be more acurate and generalistic model. However it is importante to note that even a\n",
    "Random Forest would have trouble with outliers and rare events on our dataset. Depending of our problem another model such as neural network would be more\n",
    "appropriate.\n",
    "\n",
    "One single DT would lead to over-fit model if the dataset is huge (i.e. one person's POV)\n",
    "However, if we have a voting mechanism and ask different individuals/trees to interpret the data then we would be able to cover the patterns in a much meticulous way. This is with the case of RF.\n",
    "\n",
    "Decision trees work well when the data is less complex and the splits are easily determinable. Random forests would be helpful in cases where the data is comparatively large and number of features huge. Also, random forests can handle well, noisy and missing data as compared to decision trees.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When would you prefer decision tree?\n",
    "\n",
    "DTs does not require much of data preprocessing and it does not require any assumptions of distribution of data. This algorithm is very useful to identify the hidden pattern in the dataset. When the data is simple with less features, decision tree might be useful, Otherwise Random Forest will give better predictions.\n",
    "\n",
    "Random forests can use random subset of features and/or samples for it's trees whereas decision tree do not. We prefer the Decision Tree to the Random Forest when the interpretability is more important than the accuracy. We can easily visualize our Decision Tree and understand the decision-sequence for prediction of this machine learning algorithm when we want to describe model for business users. With Random Forest we can visualize one, two or all trees in forest, but we can't understand the summary decision-sequence for whole forest.\n",
    "\n",
    "- You want visuals => DT\n",
    "- You want POWER => RF\n",
    "\n",
    "Therefore a decision tree would be better if a simple and fast model already meets our current EDA and prediction demands. However, it is model that is very prone to overfitting. If we need to analyse a bigger dataset, with a lot of features with a higher demand for accuracy a Random Forest would be a better choice.\n",
    "By bootstraping over our dataset, a Random Forest would be more accurate and generalistic model. However it is important to note that even a Random Forest would have trouble with outliers and rare events on our dataset. Depending of our problem another model such as neural network would be more appropriate.\n",
    "\n",
    "DecisionTrees are preferred over RandomForests in few cases:-\n",
    "- 1. When you have less training data. As we know that Random forest is the ensembling of Decision trees having the less data and trying to implement the Random Forest can cause overfitting as it tries to match each and every input in input dataset\n",
    "- 2. When the computational power is limited.\n",
    "- 3. When we want the model to be simple and explainable even to the business users.\n",
    "\n",
    "Decision tree advantages:\n",
    "- 1.) Easy to understand and easy to implement.\n",
    "- 2.) Runs fast.\n",
    "- 3.) Scales well with large datasets.\n",
    "- 4.) Works on numerical and categorical data.\n",
    "- 5.) Algorithm workings can be observed, so work can be reproduced.\n",
    "\n",
    "Decision tree disadvantages:\n",
    "- 1.) Prone to overfitting. Setting a max depth solves overfitting but introduces bias.\n",
    "- 2.) Running decision tree algorithms in a reasonable timespan requires greedy algorithms, which produces local optimum instead of global optimum.\n",
    "\n",
    "Decision tree is better than random forest if speed is critical and accuracy can be traded off.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You are given a data set for classification, which model would you use and why: Logistic Regression, SVM or Neural Networks?\n",
    "\n",
    "In general, it depends on the kind of data and amount of samples x features. \n",
    "\n",
    "For text classification/categorization:  I would recommend to use naive Bayes or linear SVM.\n",
    "\n",
    "For datasets with numerical attributes: I would suggest linear SVM, neural networks or logistic regression if the amount of features is much greater than the number of samples.\n",
    "\n",
    "On the other hand, I would recommend neural networks or SVM with RBF or polynomial kernel if the amount of samples is not too large and greater than the number of features.\n",
    "\n",
    "Otherwise, if the number of samples is huge I would suggest to use neural networks or linear SVM, and so on. ANN requires large data-set for training .\n",
    "\n",
    "SVMs are really good when you have a high dimensionality dataset and you don't have a lot of data.as it can handle high dimensional data or a data-set with high number of features. SVM is better for those situations where data-set is not too large. SVM without kernel is also a linear classifier.\n",
    "\n",
    "Logistic Regression is best when you have linear/Binary classification problems. LR is a very good all-purpose algorithm, if you need probabilities or you have a lot of data LR is usually good.\n",
    "\n",
    "- reduced number of features => LR\n",
    "- a lot of features but not a lot of data => SVM\n",
    "- a lot of features and a lot of data => NN\n",
    "\n",
    "Linear SVM (with liner kernel) and LR are classification methods with linear decision boundaries\n",
    "LR produces probabilistic values while SVM produces 1 or 0\n",
    "SVMs are great for relatively small data sets with fewer outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q Do you think 50 small decision trees are better than a large one? Why?__\n",
    "\n",
    "Yes!\n",
    "More robust model (ensemble of weak learners that come and make a strong learner)\n",
    "Better to improve a model by taking many small steps than fewer large steps\n",
    "\n",
    "If one tree is erroneous, it can be auto-corrected by the following.\n",
    "Less prone to overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging and Boosting\n",
    "\n",
    "Bagging and random forests are “bagging” algorithms that aim to reduce the complexity of models that overfit the training data. \n",
    "Bagging (stands for Bootstrap Aggregating) is a way to decrease the variance of your prediction by generating additional data for training from your original dataset using combinations with repetitions to produce multisets of the same cardinality/size as your original data. By increasing the size of your training set you can't improve the model predictive force, but just decrease the variance, narrowly tuning the prediction to expected outcome.\n",
    "In contrast, boosting is an approach to increase the complexity of models that suffer from high bias, that is, models that underfit the training data.\n",
    "\n",
    "__Bagging (Bootstrap AGGregatING)__\n",
    "- Before understand Bagging lets understand the concept of Bootstrap which is nothing but choosing a Random sample with replacement.\n",
    "- parallel ensemble: each model is built independently\n",
    "- aim to decrease variance, not bias\n",
    "- suitable for high variance low bias models (complex models)\n",
    "- an example of a tree based method is random forest, which develop fully grown trees (note that RF modifies the grown procedure to reduce the correlation between trees)\n",
    "\n",
    "\n",
    "\n",
    "Generate n different bootstrap training sample\n",
    "Train Algorithm on each bootstrapped sample separately\n",
    "Average the predictions at the end\n",
    "One of the Key differences is the way how use sample each training set. Bagging allows replacement in bootstrapped sample but Boosting doesn’t.\n",
    "\n",
    "__Boosting__\n",
    "- sequential ensemble: try to add new models that do well where previous models lack\n",
    "- aim to decrease bias, not variance\n",
    "- suitable for low variance high bias models\n",
    "- an example of a tree based method is gradient boosting\n",
    "- The difference from Bagging is that later model is trying to learn the error made by previous one, for example GBM and XGBoost, which eliminate the variance but have overfitting issue\n",
    "- GBDT Algorithms are Xgboost,LightGBM and Gradient Boosting Trees\n",
    "- XGBoost ias also called regularized boosting techninque  which helps to reduce overfitting\n",
    "\n",
    "\n",
    "__Bootstrap Sampling__\n",
    "\n",
    "- Bootstrap sampling means drawing random samples from training set with replacement. if our training set consists of 7 training samples, our bootstrap samples (here: n=7)\n",
    "- it produces multisets of the same cardinality/size as your original data.\n",
    "- eg2. So if i have ABCDE as my values, i can bootstrap with AABCD, etc. I can use values twice, that is the key\n",
    "- It can be used to estimate summary statistics such as the mean or standard deviation.\n",
    "\n",
    "__Stacking__\n",
    "- Bagging and boosting tend to use many homogeneous models and Stacking combines results from heterogenous model types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__How Decision Tree nodes are split and How do you determine the best split?__\n",
    "\n",
    "by using impurity ( are measures of teh homogeneity of the labels at node).  Decision tree uses entropy and information gain to select a feature which gives the best split. Intuitively, we can say that best split is when we can separate the classes accurately based on that feature.\n",
    "\n",
    "__How many nodes are there in a decision tree?__\n",
    "\n",
    "A decision tree typically starts with a single node, which branches into possible outcomes. Each of those outcomes leads to additional nodes, which branch off into other possibilities. This gives it a treelike shape. There are three different types of nodes: chance nodes, decision nodes, and end nodes.\n",
    "\n",
    "__How does Random Forest reduces the variance?__\n",
    "\n",
    "Random Forest is a ensemble bagging algorithm to achieve low prediction error. It reduces the variance of the individual decision trees by randomly selecting trees and then either average them or picking the class that gets the most vote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
