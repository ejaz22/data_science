{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Topic models\n",
    "- NMF : Non-negative Matrix Factorization\n",
    "- LDA : Latend Dirichlet Allocation\n",
    "- Cosine Similarity\n",
    "- word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply calculating the frequency of terms as in document-term matrix suffers from a critical problem, all terms are considered equally important when it comes to assessing relevancy on a query.\n",
    "\n",
    "__Named Entity Recognition (NER)__\n",
    "\n",
    "The process of locating and classifying elements in text into predefined categories such as the names of people, organizations, places, monetary values, percentages, etc.\n",
    "\n",
    "__Tokenization__\n",
    "\n",
    "- splitting a string into a list of \"tokens\" / words.\n",
    "\n",
    "__Normalization__\n",
    "\n",
    "Normalization generally refers to a series of related tasks meant to put all text on a level playing field: converting all text to the same case (upper or lower), removing punctuation, expanding contractions, converting numbers to their word equivalents, and so on. Normalization puts all words on equal footing, and allows processing to proceed uniformly.\n",
    "\n",
    "__Stemming__\n",
    "\n",
    "Stemming is the process of eliminating affixes (suffixed, prefixes, infixes, circumfixes) from a word in order to obtain a word stem.\n",
    "\n",
    "running ===> run\n",
    "\n",
    "__Lemmatization__\n",
    "\n",
    "Lemmatization is related to stemming, differing in that lemmatization is able to capture canonical forms based on a word's lemma. For example, stemming the word \"better\" would fail to return its citation form (another word for lemma); however, lemmatization would result in the following:\n",
    "\n",
    "better ===> good\n",
    "\n",
    "\n",
    "__Stop Words__\n",
    "\n",
    "Stop words are those words which are filtered out before further processing of text, since these words contribute little to overall meaning, given that they are generally the most common words in a language. For instance, \"the,\" \"and,\" and \"a,\" \n",
    "\n",
    "__Dcoument__\n",
    "\n",
    "- a document is a set of terms.\n",
    "\n",
    "__Corpus__\n",
    "\n",
    "- a corpus is a set of document.\n",
    "\n",
    "__Bag of Words (BoW).__\n",
    "\n",
    "- bag referring to the set theory concept of multisets, which differ from simple sets\n",
    "- The bag of words model omits grammar and word order, but is interested in the number of occurrences of words within the text.\n",
    "- BoW is simply a word occurence as how many times a word appears in a document\n",
    "- BoW has limitations such as large feature dimension, sparse representation etc. \n",
    "- if dataset is small and context is domain specific, BoW may work better than Word Embedding\n",
    "- In simple bag-of-words (e.g uni-gram, bi-gram, tri-gram) representations,the frequency (or asimilar weight such as term frequency inverse document frequency) of each word or n-gram is considered as a separate feature. \n",
    "- Bag of Words (BoW) is an algorithm that counts how many times a word appears in a document. \n",
    "- Bag of words models encode every word in the vocabulary as one-hot-encoded vector \n",
    "- Bag of word models don’t respect semantics of the word. For example: words ‘car’ and ‘automobile’ are often used in the same context.\n",
    "- While modeling phrases using bag-of-words the order of words in the phrase is not respected. Ex: “This is good” and “Is this good” have exactly the same vector representation. BoW lose all information about word order: “John likes Mary” and “Mary likes John” correspond to identical vectors. There is a solution: bag of n-grams models consider word phrases of length n to represent documents as fixed-length vectors to capture local word order but suffer from data sparsity and high dimensionality. The Word2Vec model addresses this second problem.\n",
    "\n",
    "- CountVectorizer()\n",
    "\n",
    "\n",
    "__Term Frequency(TF)__\n",
    "\n",
    "- Normalize count occurence\n",
    "- TF-IDF approach believe that high frequency may not able to provide much information gain. In another words, rare words contribute more weights to the model.\n",
    "- It is the ratio of number of times a word occurred in a document to the total number of words in the document.\n",
    "- TfidfVectorizer()\n",
    "\n",
    "__Inverse Document Frequency(IDF)__\n",
    "\n",
    "-  IDF (inverse document frequency) assumes that the importance of a term is inversely proportional to the frequency of occurrence of this term in all the documents \n",
    "\n",
    "- it is the logarithm of (total number of documents divided by number of documents containing the word).\n",
    "\n",
    "__TF-IDF__\n",
    "\n",
    "- tf-idf helps you rank the importance of a term\n",
    "- With TF-IDF, words are given weight – TF-IDF measures relevance, not frequency\n",
    "- Term frequency (tf) is basically the output of the BoW model\n",
    "\n",
    "- With TF-IDF, words are given weight – TF-IDF measures relevance, not frequency. \n",
    "- TF-IDF can achieve better results than simple term frequencies when combined with some supervised methods.\n",
    "- TfidfVectorizer Converts a collection of raw documents to a matrix of TF-IDF features. \n",
    "- Tfidfvectorizer are feature generation algorithms and hence running on test will not cause overfitting or leakage\n",
    "- Tf-idf is a scoring scheme for words - that is a measure of how important a word is to a document.\n",
    "- TF-IDF is a way to judge the topic of an article. This is done by the kind of words it contains. Here words are given weight so it measures relevance, not frequency.\n",
    "\n",
    "\n",
    "__Word2Vec__\n",
    "\n",
    "- also known as Skip-gram with Negative Sampling (SGNS)\n",
    "- Word2vec produces one vector per word, whereas tf-idf produces a score. \n",
    "- Word2vec produces one vector per word, whereas BoW produces one number (a wordcount)\n",
    "- word2vec learns relationships between words automatically.\n",
    "- Gensim is heavily applied for training word2vec and doc2vec\n",
    "- can solve analogies cleary\n",
    "- gensim.models.Word2Vec()\n",
    "\n",
    "__Doc2Vec__\n",
    "\n",
    "- Doc2Vec is a Model that represents each Document as a Vector. \n",
    "\n",
    "\n",
    "__DeepIR__\n",
    "\n",
    "__Global Vectors (GloVe)__\n",
    "- GloVe is modeled to do dimensionality reduction.\n",
    "- Glove and Word2vec are both unsupervised models for generating word vectors\n",
    "\n",
    "__Cosine Similarity__\n",
    "\n",
    "- With cosine similarity we can measure the similarity between two document vectors\n",
    "- cosine similarity == 1 ==> same document. If it is 0, the documents share nothing. \n",
    "\n",
    "- compare different documents with cosine similarity or the Euclidean dot product formula.\n",
    "\n",
    "__Latent Dirichlet Allocation (LDA)__\n",
    "\n",
    "- Latent Dirichlet Allocation, LDA is yet another transformation from bag-of-words counts into a topic space of lower dimensionality. LDA is a probabilistic extension of LSA (also called multinomial PCA), so LDA’s topics can be interpreted as probability distributions over words. These distributions are, just like with LSA, inferred automatically from a training corpus. Documents are in turn interpreted as a (soft) mixture of these topics (again, just like with LSA)\n",
    "\n",
    "-  LDA model didn’t do a very good job of classifying our documents into topics. This is mostly because the corpus used to train the LDA model is so small. Using a larger corpus should give you much better results\n",
    "\n",
    "\n",
    "\n",
    "__Latent Semantic Analysis (LSA)__\n",
    "\n",
    "- Both LSA and LDA have same input which is Bag of words in matrix format. LSA focus on reducing matrix dimension while LDA solves topic modeling problems.\n",
    "\n",
    "__Positive Pointwise Mutual Information (PPMI)__\n",
    "\n",
    "- PMI is a typical measure for the strength of association between two words.\n",
    "\n",
    "__Singular Value Decomposition__\n",
    "\n",
    "- SVD is among the more popular methods for dimensionality reduction and came about in NLP originally via latent semantic analysis (LSA)\n",
    "- The challenge of SVD is that we are hard to determine the optimal number of dimension\n",
    "\n",
    "__Random Projections(RP)__\n",
    "\n",
    "RP aim to reduce vector space dimensionality. This is a very efficient (both memory- and CPU-friendly) approach to approximating TfIdf distances between documents, by throwing in a little randomness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q. What is NLP preprocessing steps?__\n",
    "\n",
    "We first clean the text by removing HTML tags. Next, we tokenize the message’s sentences and remove stopwords. Then, we conduct lemmatization to convert words in different inflected forms into the same base form. Finally, we convert the documents into a collection of words (a so-called bag of words) and build a dictionary of those words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there are two kinds of works involved in text representation: indexing and term weighting. Indexing is the job to assign the indexing terms for the documents\n",
    "\n",
    "Term weighting is the job to assign the weights of terms which measure the importance of terms in documents\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q 19. What is: collaborative filtering, n-grams, cosine distance?__\n",
    "\n",
    "Collaborative filtering:\n",
    "- Technique used by some recommender systems\n",
    "- Filtering for information or patterns using techniques involving collaboration of multiple agents: viewpoints, data sources.\n",
    "1. A user expresses his/her preferences by rating items (movies, CDs.)\n",
    "2. The system matches this user’s ratings against other users’ and finds people with most similar tastes\n",
    "3. With similar users, the system recommends items that the similar users have rated highly but not yet being rated by this user\n",
    "\n",
    "n-grams:\n",
    "- Contiguous sequence of n items from a given sequence of text or speech\n",
    "- “Andrew is a talented data scientist”\n",
    "- Bi-gram: “Andrew is”, “is a”, “a talented”.\n",
    "- Tri-grams: “Andrew is a”, “is a talented”, “a talented data”.\n",
    "- An n-gram model models sequences using statistical properties of n-grams; see: Shannon Game\n",
    "- More concisely, n-gram model: P(Xi|Xi−(n−1)...Xi−1): Markov model\n",
    "- N-gram model: each word depends only on the n−1 last words\n",
    "\n",
    "Issues:\n",
    "- when facing infrequent n-grams\n",
    "- solution: smooth the probability distributions by assigning non-zero probabilities to unseen words or n-grams\n",
    "- Methods: Good-Turing, Backoff, Kneser-Kney smoothing\n",
    "\n",
    "Cosine distance:\n",
    "- How similar are two documents?\n",
    "- Perfect similarity/agreement: 1\n",
    "- No agreement : 0 (orthogonality)\n",
    "- Measures the orientation, not magnitude\n",
    "\n",
    "Given two vectors A and B representing word frequencies:\n",
    "cosine-similarity(A,B)=⟨A,B⟩||A||⋅||B||\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q. How would you come up with a solution to identify plagiarism?__\n",
    "\n",
    "- Vector space model approach\n",
    "- Represent documents (the suspect and original ones) as vectors of terms\n",
    "- Terms: n-grams; n=1 to as much we can (detect passage plagiarism)\n",
    "- Measure the similarity between both documents\n",
    "- Similarity measure: cosine distance, Jaro-Winkler, Jaccard\n",
    "- Declare plagiarism at a certain threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

