{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Metrics\n",
    "\n",
    "\n",
    "- RMSE (root mean squared error), \n",
    "- MAE (mean absolute error), \n",
    "- WMAE(weighted mean absolute error), \n",
    "- RMSLE (root mean squared logarithmic error)…\n",
    "\n",
    "\n",
    "\n",
    "In regression model, the most commonly known evaluation metrics include:\n",
    "\n",
    "__R-squared (R2)__, which is the proportion of variation in the outcome that is explained by the predictor variables. In multiple regression models, R2 corresponds to the squared correlation between the observed outcome values and the predicted values by the model. The Higher the R-squared, the better the model.\n",
    "\n",
    "__Root Mean Squared Error (RMSE)__\n",
    "\n",
    "- RMSE follows an assumption that error are unbiased and follow a normal distribution. As compared to mean absolute error, RMSE gives higher weightage and punishes large errors.\n",
    "\n",
    "- Because the MSE is squared, its units do not match that of the original output. RMSE is the square root of MSE.\n",
    "- Since the MSE and RMSE both square the residual, they are similarly affected by outliers.\n",
    "- The RMSE is analogous to the standard deviation and is a measure of how large the residuals are spread out.\n",
    "- Generally, RMSE will be higher than or equal to MAE. The lower the RMSE, the better the model.\n",
    "\n",
    "__Residual Standard Error (RSE)__, also known as the model sigma, is a variant of the RMSE adjusted for the number of predictors in the model. The lower the RSE, the better the model. In practice, the difference between RMSE and RSE is very small, particularly for large multivariate data.\n",
    "\n",
    "__Mean Absolute Error (MAE)__, like the RMSE, the MAE measures the prediction error. Mathematically, it is the average absolute difference between observed and predicted outcomes, MAE = mean(abs(observeds - predicteds)). MAE is less sensitive to outliers compared to RMSE.\n",
    "\n",
    "- Average of the difference between the Original Values and the Predicted Values.\n",
    "- Do not gives any idea of the direction of the error i.e. whether we are under predicting the data or over predicting the data.\n",
    "- Smaller the MAE, better is the model.\n",
    "- Robust to outliers\n",
    "- Range (0, + infinity]\n",
    "\n",
    "__Mean Squared Error__\n",
    "\n",
    "- Takes the average of the square of the difference between the original values and the predicted values.\n",
    "- As we take square of the error, the effect of larger errors(sometimes outliers) become more pronounced then smaller error. Model will be penalized more for making predictions that differ greatly from the corresponding actual value.\n",
    "- Before applying MSE, we must eliminate all nulls/infinites from the input.\n",
    "- Not robust to outliers\n",
    "- Range (0, + infinity]\n",
    "\n",
    "__Root Mean Squared Logarithmic Error__\n",
    "\n",
    "- We take the log of the predictions and actual values.\n",
    "- What changes are the variance that we are measuring.\n",
    "- RMSLE is usually used when we don’t want to penalize huge differences in the predicted and the actual values when both predicted and actual values are huge numbers.\n",
    "- If both predicted and actual values are small: RMSE and RMSLE are same.\n",
    "- If either predicted or the actual value is big: RMSE > RMSLE\n",
    "- If both predicted and actual values are big: RMSE > RMSLE (RMSLE becomes almost negligible)\n",
    "\n",
    "The problem with the above metrics, is that they are sensible to the inclusion of additional variables in the model, even if those variables dont have significant contribution in explaining the outcome. Put in other words, including additional variables in the model will always increase the R2 and reduce the RMSE. So, we need a more robust metric to guide the model choice.\n",
    "\n",
    "Concerning R2, there is an adjusted version, called Adjusted R-squared, which adjusts the R2 for having too many variables in the model.\n",
    "\n",
    "Additionally, there are four other important metrics - AIC, AICc, BIC and Mallows Cp - that are commonly used for model evaluation and selection. These are an unbiased estimate of the model prediction error MSE. The lower these metrics, he better the model.\n",
    "\n",
    "__AIC stands for (Akaike’s Information Criteria)__, a metric developped by the Japanese Statistician, Hirotugu Akaike, 1970. The basic idea of AIC is to penalize the inclusion of additional variables to a model. It adds a penalty that increases the error when including additional terms. The lower the AIC, the better the model.\n",
    "AICc is a version of AIC corrected for small sample sizes.\n",
    "BIC (or Bayesian information criteria) is a variant of AIC with a stronger penalty for including additional variables to the model.\n",
    "\n",
    "Mallows Cp: A variant of AIC developed by Colin Mallows.\n",
    "\n",
    "\n",
    "__Common metrics in regression:__\n",
    "\n",
    "__Mean Squared Error Vs Mean Absolute Error RMSE__ gives a relatively high weight to large errors. The RMSE is most useful when large errors are particularly undesirable.\n",
    "\n",
    "__The MAE is a linear score:__ all the individual differences are weighted equally in the average. MAE is more robust to outliers than MSE.\n",
    "\n",
    "RMSE=1n∑ni=1(yi−y^i)2−−−−−−−−−−−−−−√\n",
    "MAE=1n∑ni=1|yi−y^i|\n",
    "\n",
    "__Root Mean Squared Logarithmic Error__ RMSLE penalizes an under-predicted estimate greater than an over-predicted estimate (opposite to RMSE)\n",
    "\n",
    "RMSLE=1n∑ni=1(log(pi+1)−log(ai+1))2−−−−−−−−−−−−−−−−−−−−−−−−−−−√\n",
    "Where pi is the ith prediction, ai the ith actual response, log(b) the natural logarithm of b.\n",
    "\n",
    "__Weighted Mean Absolute Error__\n",
    "The weighted average of absolute errors. MAE and RMSE consider that each prediction provides equally precise information about the error variation, i.e. the standard variation of the error term is constant over all the predictions. Examples: recommender systems (differences between past and recent products)\n",
    "WMAE=1∑wi∑ni=1wi|yi−y^i|\n",
    "\n",
    "\n",
    "minimizing the squared error over a set of numbers results in finding its mean, and minimizing the absolute error results in finding its median. This is the reason why MAE is robust to outliers whereas RMSE is not. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q Why are some scores like MSE negative in scikit-learn?__\n",
    "\n",
    "Some model evaluation metrics such as mean squared error (MSE) are negative when calculated in scikit-learn.\n",
    "\n",
    "This is confusing, because error scores like MSE cannot actually be negative, with the smallest value being zero or no error.\n",
    "\n",
    "The scikit-learn library has a unified model scoring system where it assumes that all model scores are maximized. In order this system to work with scores that are minimized, like MSE and other measures of error, the sores that are minimized are inverted by making them negative.\n",
    "\n",
    "This can also be seen in the specification of the metric, e.g. ‘neg‘ is used in the name of the metric ‘neg_mean_squared_error‘.\n",
    "\n",
    "When interpreting the negative error scores, you can ignore the sign and use them directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__MAE vs. MSE__\n",
    "\n",
    "- Being more complex and biased towards higher deviation, RMSE is still the default metric of many models because loss function defined in terms of RMSE is smoothly differentiable whereas Mean Absolute Error requires complicated linear programming to compute the gradient.\n",
    "- If we want a metric just to compare between two models from interpretation point of view, then MAE may be a better choice.\n",
    "- Units of both RMSE & MAE are same as y values which is not true for R Square.\n",
    "- Minimizing the squared error (𝐿2) over a set of numbers results in finding its mean, and minimizing the absolute error (𝐿1) results in finding its median.\n",
    "\n",
    "\n",
    "__Adjusted R² over RMSE__\n",
    "\n",
    "Absolute value of RMSE does not actually tell how good/bad a model is. It can only be used to compare across two models whereas Adjusted R² easily does that. For example, if a model has adjusted R² equal to 0.05 then it is definitely bad.\n",
    "\n",
    "However, if we care only about prediction accuracy then RMSE is best. It is computationally simple, easily differentiable and present as default metric for most of the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi class classification\n",
    "\n",
    "- Multi Class : classify a set of images of fruits into any one of these categories — apples, bananas, and oranges.\n",
    "\n",
    "- Multi label : tagging a blog into one or more topics like technology, religion, politics\n",
    "\n",
    "\n",
    "- MultiClass Classifiers can distinguish between more than two classes.\n",
    "\n",
    "Random Forest Classifiers or Naive Bayes Classifiers are capable of handling multiple classes directly. Others (Support Vector Machine classifiers or Linear classifiers) are strictly binary classifiers.\n",
    "\n",
    "Task: 0 - 9 digits classification\n",
    "One vs All (OvA) Classification Strategy : \n",
    "Train 10 binary classifiers, one for each digit (a 0-detector, a 1-detector, a 2-detector, and so on). When we want to classify an image, we get the decision score from each classifier for that image and we select the class whose classifier outputs the highest score.\n",
    "\n",
    "Example : Almost all classification algorithms.\n",
    "\n",
    "One vs One (OvO) Strategy : \n",
    "Train a binary classifier for every pair of digits: one to distinguish 0s and 1s, another to distinguish 0s and 2s, another for 1s and 2s, and so on. If there are N classes, we need to train N × (N – 1) / 2 classifiers. For the MNIST problem, this means training 45 binary classifiers. When we want to classify an image, we have to run the image through all 45 classifiers and see which class wins the most duels. Main advantage of OvO is that each classifier only needs to be trained on the part of the training set for the two classes that it must distinguish.\n",
    "\n",
    "Example : Support Vector Machines scale poorly with the size of the training set, it is faster to train many classifiers on small training sets than training few classifiers on large training sets.\n",
    "\n",
    "Scikit-Learn detects when we try to use a binary classification algorithm for a multi‐ class classification task, and it automatically runs OvA (except for SVM classifiers for which it uses OvO).For MNIST problem, Under the hood, Scikit-Learn trained 10 binary classifiers, get their decision scores for the image, and selected the class with the highest score.\n",
    "\n",
    "If we want to force ScikitLearn to use OvO or OvA, we can use the OneVsOneClassifier or OneVsRestClassifier classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Metric \n",
    "\n",
    "BLEU (Bilingual Evaluation Understudy)\n",
    "\n",
    "It is mostly used to measure the quality of machine translation with respect to the human translation. It uses a modified form of precision metric.\n",
    "\n",
    "Example: Reference: The cat is sitting on the mat\n",
    "\n",
    "Machine Translation 1: On the mat is a cat\n",
    "\n",
    "Machine Translation 2: There is cat sitting cat\n",
    "\n",
    "Machine Translation 3: The cat is sitting on the tam"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
