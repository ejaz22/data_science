{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word embeddings\n",
    "\n",
    "One hot word vectors are hard coded sparse and high dimensional whereas word embeddings are dense, low dimensional and learned fromd data.\n",
    "\n",
    "```python\n",
    "from keras.layers import Embedding\n",
    "\n",
    "# The Embedding layer takes at least two arguments:\n",
    "# the number of possible tokens, here 1000 (1 + maximum word index),\n",
    "# and the dimensionality of the embeddings, here 64.\n",
    "embedding_layer = Embedding(1000, 64)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "https://github.com/fchollet/deep-learning-with-python-notebooks\n",
    "\n",
    "## Artificial Neural Networks (ANN)\n",
    "\n",
    "An ANN is built from nodes (neurons) stacked in layers between the feature vector and the target vector.\n",
    "\n",
    "A node in a neural network is built from Weights and Activation function\n",
    "\n",
    "An early version of ANN built from one node was called the Perceptron\n",
    "\n",
    "The Perceptron is an algorithm for supervised learning of binary classifiers. functions that can decide whether an input (represented by a vector of numbers) belongs to one class or another. A Perceptron Network can be designed to have multiple layers, leading to the Multi-Layer Perceptron (aka MLP)\n",
    "\n",
    "Much like logistic regression, the weights in a neural net are being multiplied by the input vertor summed up and feeded into the activation function's input.\n",
    "\n",
    "## Backward Propagation ##\n",
    "The weights of each neuron are learned by gradient descent, where each neuron's error is derived with respect to it's weight.\n",
    "\n",
    "Optimization is done for each layer with respect to the previous layer in a technique known as BackPropagation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Function\n",
    "\n",
    "The activation functions in general is that they add a certain complexity, such that the classifier may learn to generalize. It adds non-linearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid (Logistic) \n",
    "Also known as squashing function. It takes a real-valued number as an input and compresses all its outputs to the range of [0,1] There are many functions with the characteristic of an “S” shaped curve known as sigmoid functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid (Tanh :Hyperbolic Tangent)\n",
    "#### Tangens hyperbolicus: Tanh\n",
    "\n",
    "Tanh is a non-linear activation function for deep learning that compresses all its inputs to the range [-1, 1]. \n",
    "\n",
    "However, the tanh function could not solve the vanishing gradient problem suffered by the sigmoid functions as well. The main advantage of this function is that it produces zero centred output thereby aiding the back-propagation process.\n",
    "\n",
    "A property of the tanh function is that it can only attain a gradient of 1, only when the value of the input is 0, that is when x is zero. This makes the tanh function produce some dead neurons during computation. The dead neuron is a condition where the activation weight, rarely used as a result of zero gradient. This limitation of the tanh function spurred further research in activation functions to resolve the problem, and it birthed the rectified linear unit (ReLU) activation function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax function\n",
    "\n",
    "It is used to compute probability distribution from a vector of real numbers. The Softmax function produces an output which is a range of values between 0 and 1, with the sum of the probabilities been equal to 1.\n",
    "\n",
    "The Softmax function is used in multi-class models where it returns probabilities of each class, with the target class having the highest probability.\n",
    "\n",
    "\n",
    "The main difference between the Sigmoid and Softmax is that the Sigmoid is used in binary classification while the\n",
    "Softmax is used for multivariate classification task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLu\n",
    "\n",
    "Relu (=max(0,x)) is used to extract feature maps from data. This is why it is used in the hidden layers where we're learning what important characteristics or features the data holds that could make the model learn how to classify for example. In the FC layers, it's time to make a decision about the output, so we usually use sigmoid or softmax, which tend to give us numbers between 0 and 1 (probability) that can give an interpretable result.\n",
    "\n",
    "Sigmoid gives a probability for each class. So, if you have 10 classes, you'll have 10 probabilities. And depending on the threshold used, your model would predict for example that the image corresponds to two classes when in multi-classification you want just one predicted class per image. That's why softmax is used in this context: It chooses the class with the maximum probability. So it'll predict just one class.\n",
    "\n",
    "\n",
    " you will face the dying ReLU problem (Jaideep, n.d.). If a neuron’s weights are moved towards the zero output, it may be the case that they eventually will no longer be capable of recovering from this. They will then continually output zeros. This is especially the case when your network is poorly initialized, or when your data is poorly normalized, because the first rounds of optimization will produce large weight swings. When too many neurons output zero, you end up with a dead neural network – the dying ReLU problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Optimizers are algorithms or methods used to change the attributes of your neural network such as weights and learning rate in order to reduce the losses.\n",
    "\n",
    "Gradient Descent: It is the most basic but most used optimization algorithm.\n",
    "\n",
    "Stochastic gradient descent (SGD): It’s a variant of Gradient Descent. It tries to update the model’s parameters more frequently. In this, the model parameters are altered after computation of loss on each training example.\n",
    "\n",
    "Momentum: It was invented for reducing high variance in SGD and softens the convergence. This method helps accelerate SGD in the relevant direction.\n",
    "\n",
    "Adam (Adaptive Moment Estimation):The intuition behind the Adam is that we don’t want to roll so fast just because we can jump over the minimum, we want to decrease the velocity a little bit for a careful search.\n",
    "\n",
    "AdaGrad: It is an adaptive method for setting the learning rate.\n",
    "\n",
    "RMSProp: It modifies AdaGrad by changing the gradient accumulation into an exponentially weighted moving average, i.e. it discards history from the distant past."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
