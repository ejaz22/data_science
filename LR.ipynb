{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression / Logit Regression / maximum-entropy classification (MaxEnt) / the log-linear classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- How will you deal with the multiclass classification problem using logistic regression?\n",
    "- Explain the use of ROC curves and the AUC of an ROC Curve.\n",
    "- How can you use the concept of ROC in a multiclass classification?\n",
    "- How to visualize decision boundary.\n",
    "- How to create polynomial and interactions terms\n",
    "- How to visualize learning curve\n",
    "- How to read classification report\n",
    "- l1 and l2 term regularization\n",
    "- what is jaccard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistics Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression an extension of Linear regression where the dependent variable is categorical and not continuous. Multinomial and Binary Logistic Regression is estimated using Maximum Likelihood Estimation (MLE), unlike linear regression which uses the Ordinary Least Squares (OLS) approach.\n",
    "\n",
    "Logistic regression is based on Maximum Likelihood (ML) Estimation which says coefficients should be chosen in such a way that it maximizes the Probability of Y given X (likelihood). With ML, the computer uses different \"iterations\" in which it tries different solutions until it gets the maximum likelihood estimates. Fisher Scoring is the most popular iterative method of estimating the regression parameters.\n",
    "\n",
    "The output of a Logistic regression model is a probability. We can select a threshold value. If the probability is greater than this threshold value, the event is predicted to happen otherwise it is predicted not to happen. A suitable cut off point is chosen based on ROC curve.\n",
    "\n",
    "In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the 'multi_class' option is set to 'ovr' and uses the cross-entropy loss, if the 'multi_class' option is set to 'multinomial'. (Currently the 'multinomial' option is supported only by the 'lbfgs' and 'newton-cg' solvers.)\n",
    "\n",
    "__Assumptions__\n",
    "\n",
    "- Dependent variable is binary, \n",
    "- Observations are independent of each other, \n",
    "- Little or no multicollinearity among the independent variables, \n",
    "- Linearity of independent variables and log odds.\n",
    "\n",
    "__Limitation__\n",
    "\n",
    "Classic logistic regression doesn't handle nonlinearities well (however you can include interaction and polynomial terms).\n",
    "\n",
    "\n",
    "__Why Logistics Regression__\n",
    "\n",
    "- Logistic Regression does well at modeling variables but often struggles to capture interactions between variables. Other models (e.g., XGB) do very well at capturing interactions\n",
    "\n",
    "- linear models work best with dummiesvariables, but other algorithms like XGBoost and Trees don't rely a lot on dummies variables.\n",
    "\n",
    "- OHE should be the best encoding for regressions, the danger of one-hot encoding will be overfitting the training set as a result of either incomplete sampling of the feature space or noise\n",
    "\n",
    "__For Effective LR__\n",
    "- Remove excess collinearity (by applying PCA) \n",
    "- Normalise the variables (using Standard or Robust Scaler), you can normalize all your features to the same scale before putting them in a machine learning model.\n",
    "- Interaction terms : Investigate factors for linear relationship.\n",
    "- Apply transformations as required to remove outliers\n",
    "- removing features (eg. using RFE)\n",
    "- using a non- liner model\n",
    "- Class Imbalance - Look for class imbalance in your data. Since you are working with admit/reject data, then the number of rejects would be significantly higher than the admits. Most classifiers in SkLearn including LogisticRegression have a class_weight parameter. Setting that to balanced might also work well in case of a class imbalance.\n",
    "- SVM can able to learn more complex decision boundaris\n",
    "- logistic regression cannot deal with missing values. Therefore all incomplete cases will be excluded during the estimation process. In order to avoid that you would have to impute the missing values (or substitute them with a mean or median).\n",
    "\n",
    "\n",
    "__Model Performance__\n",
    "\n",
    "a model performance depends on dataset and its peroperties such as its distribution. Simple model like LR for binary classification has higher generalization over boosting.\n",
    "\n",
    "\n",
    "__LR Hyperparameters__\n",
    "\n",
    "Regularization is a very useful method to handle collinearity (high correlation among features) and prevents overfitting. Regulariztion introduces addditiuonal information (bias) to penalize parameter weights. Regularization works when features are on comparable scales. Decision regions change when using different regularization values.\n",
    "\n",
    "\n",
    "C = 1/λ\n",
    "\n",
    "C which is a inverse regularization parameter, controls the amount of overfitting (a lower value should decrease overfitting). \n",
    "From Validation curve, choose  C which offers the smallest difference between the training and testing accuracy. ( our aim is to generalize model on unseen data)\n",
    "\n",
    "penalty : [L1, L2]\n",
    "\n",
    "- L2 produces moldels with many samll coeff. whereas L1 with large number of zero  coefficients.\n",
    "- All coefficients consistently get smaller in size as L2 penalty is increased.\n",
    "\n",
    "__Alogrith for Optimization Alogrithms__\n",
    "\n",
    "the optimization algorithm  iteratively updates the weights so as to minimize this loss function. The standard algorithm for this is gradient descent; ( stochastic gradient descent )\n",
    "\n",
    "Our goal with gradient descent is to find the optimal weights: minimize the loss function. For logistic regression, this loss function is conveniently convex. A convex function has just one minimum; there are no local minima to get stuck in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver_options = ['newton-cg', 'lbfgs', 'liblinear', 'sag']\n",
    "multi_class_options = ['ovr', 'multinomial']\n",
    "class_weight_options = ['None', 'balanced']\n",
    "C = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "penalty = ['l1','l2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__liblinear__\n",
    "- For small datasets, 'liblinear' is a good choice, whereas 'sag' is faster for large ones.\n",
    "- Regularize the intercept, which is wrong. We should avoid it when fit_intercept=True.\n",
    "- Does not handle multi_class='multinomial'.\n",
    "- Only solver which handles dual=True.\n",
    "\n",
    "__Stochastic Average Gradient (saga)__\n",
    "\n",
    "- only solver which handles penalty='l1' with multi_class='multinomial'.\n",
    "- Very fast for large datasets, dense or sparse.\n",
    "- May suffer slower convergence when the features are not scaled.\n",
    "- May suffer slower convergence when there are large norm outlier samples. (This might be improved with a better step-size heuristic (e.g.), or with an adaptive step-size.)\n",
    "\n",
    "__newton-cg__\n",
    "\n",
    "- Newton’s method uses in a sense a better quadratic function minimisation. A better because it uses the quadratic approximation (i.e. first AND second partial derivatives).\n",
    "\n",
    " __Limited-memory Broyden–Fletcher–Goldfarb–Shanno (lbfgs)__\n",
    "- Good default, but not the fastest for large datasets.\n",
    "- Does not handle penalty='l1'.\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q. How to Optimize logistics Regression Model?__\n",
    "\n",
    "Use paramter C as our regularization parameter. C = 1/λ.\n",
    "\n",
    "Lambda (λ) controls the trade-off between allowing the model to increase it's complexity as much as it wants with trying to keep it simple. \n",
    "\n",
    "- Higher C is more likely to overfit.\n",
    "\n",
    "For example, if λ is very low or 0, the model will have enough power to increase it's complexity (overfit) by assigning big values to the weights for each parameter. If, on the other hand, we increase the value of λ, the model will tend to underfit, as the model will become too simple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q Why can't linear regression be used in place of logistics regression?__\n",
    "\n",
    "- Distribution of error terms: The distribution of data in case of linear and logistic regression is different. Linear regression assumes that error terms are normally distributed. In case of binary classification, this assumption does not hold true.\n",
    "\n",
    "\n",
    "- Model output: In linear regression, the output is continuous. In case of binary classification, an output of a continuous value does not make sense. For binary classification problems, linear regression may predict values that can go beyond 0 and 1. If we want the output in the form of probabilities, which can be mapped to two different classes, then its range should be restricted to 0 and 1. As the logistic regression model can output probabilities with logistic/sigmoid function, it is preferred over linear regression.\n",
    "\n",
    "\n",
    "- Variance of Residual errors: Linear regression assumes that the variance of random errors is constant. This assumption is also violated in case of logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q What is the difference in SGD and LR?__\n",
    "\n",
    "Logistic regression classifier has different solvers and one of them is 'sgd' (Stochastic Gradient Descent)\n",
    "\n",
    "- SGD is a optimization method, while Logistic Regression (LR) is a machine learning algorithm/model. A machine learning model defines a loss function, and the optimization method minimizes/maximizes it.\n",
    "- sklearn SGDClassfier is liner classifier optimized by SGD\n",
    "- LR can use other optimizers like L-BFGS, conjugate gradient or Newton-like methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we compare the model predictions from a test set to the actual, ground truth results\n",
    "\n",
    "In some scenarios, we are ok with the overall accuracy whereas in some scenario the cost of misclassifying a single data point is huge. For example In a scenario of bank finding whether a customer is eligible for the loan or not it can be alright if we might misclassify as some eligible customers as not eligible. But in case of a doctor classifying the patients as having cancer or not it would be a blunder if we declare some potential cancer patients as cancer-free.\n",
    "\n",
    "__Confusion Matrix__\n",
    "\n",
    "A confusion matrix is a table that is often used to describe the performance of a classification model on a set of test data for which the true values are known.\n",
    "\n",
    "\n",
    "__Basic terms related to Confusion matrix or contingency table:__\n",
    "- a) True Positives: Observations where the actual and predicted transactions were fraud\n",
    "\n",
    "- b) True Negatives: Observations where the actual and predicted transactions weren’t fraud\n",
    "\n",
    "- c) False Positives: Observations where the actual transactions weren’t fraud but predicted to be fraud\n",
    "\n",
    "- d) False Negatives: Observations where the actual transactions were fraud but weren’t predicted to be fraud\n",
    "- ideal scenario - zero values for FP and FN\n",
    "- Samples in the FP set are actually negatives and samples in FN are actually positives.\n",
    "\n",
    "__Accuracy (Acc)__\n",
    "\n",
    "- Classification accuracy is the percentage of correct prediction over total instances.\n",
    "- = (tp + tn) / (tp + tn + fp + fn)\n",
    "- Accuracy is a good metric if the classes are balanced\n",
    "-  it doesn't tell us where the model is making errors. Answering this \"where\" question is an essential part of model-building. \n",
    "\n",
    "__Classification Error / Error Rate / Misclassification Rate (ERR)__\n",
    "\n",
    " - measures the ratio of incorrect predictions over the total number of instances evaluated. \n",
    " - = (fp + fn) / (tp + tn + fp + fn)\n",
    " - = ERR = (1-Acc)\n",
    "\n",
    "- applicable for multi-class and multi-label problems;\n",
    "- Another problem with the accuracy is that two classifiers can yield the same accuracy but perform differently\n",
    "- Both accuracy and error rate metrics are sensitive to the imbalanced data. The imbalance dataset makes accuracy, not a reliable performance metric to use. To cope with this problem, we can choose to penalize false positives or false negatives. This will generate two alternative metrics i.e precision and recall.\n",
    "\n",
    "\n",
    "__Recall, Precision and F1__\n",
    "\n",
    "- The precision is the ability of the classifier not to label as positive when it is negative. With precision, we are evaluating our data by its performance of ‘positive’ predictions.\n",
    "- Precision is the probability that our system will properly identify as positive.\n",
    "- Precision = TP / (TP + FP)\n",
    " \n",
    "- The recall is the ability of the classifier to find all the positive samples. With recall, we are evaluating our data by its performance of the ground truths for positive outcomes.\n",
    "- e.g. If a sample is positive for the disease, what’s the probability that the system will pick it up\n",
    "- Recall = TP / (TP + FN)\n",
    "\n",
    "\n",
    "__Sensitivity and Specificity__\n",
    "- Sensitivity is True Positive rate and is also called recall\n",
    "- Specificity (TNR) is the opposite of Recall. Hence, the formula is TN/(TN+FP).\n",
    "- Sensitivity and Specificity may give you a biased result, especially for imbalanced classes.\n",
    "- both precision and recall are necessary to determine if the classifier is performing well.\n",
    "- The F1 score can be interpreted as a weighted average of the precision and recall,\n",
    "- F1 is used where true negatives don’t matter much\n",
    "- The best value for recall, precision and F1 is 1 and the worst value is 0\n",
    "\n",
    "\n",
    "__Matthews Correlation Coefficient (MCC)__\n",
    "- Similar to Correlation Coefficient, the range of values of MCC lie between -1 to +1. A model with a score of +1 is a perfect model and -1 is a poor model. This property is one of the key usefulness of MCC as it leads to easy interpretability.\n",
    "\n",
    "\n",
    "__ROC Curve__\n",
    "\n",
    "- A ROC(Receiver Operator Characteristic Curve) is a graphical assesment method which can help in deciding the best threshold value. \n",
    "- It shows the performance of a classification model at all classification thresholds.\n",
    "- The ROC Curve/AUC Score is most useful when we are evaluating a model to itself\n",
    "- It is generated by plotting the True Positive Rate (y-axis) against the False Positive Rate (x-axis)\n",
    "\n",
    "\n",
    "__AOC__\n",
    "- AUC is one of the popular ranking type metrics\n",
    "\n",
    "- The area under ROC is called Area Under the Curve(AUC). AUC gives the rate of successful classification by the logistic model.\n",
    "\n",
    "__Choice of Metrics__\n",
    "\n",
    "It depends on the business objective. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using cross validation for more robust error measurement\n",
    "Using a Validation dataset has a drawback. Firstly, it decreases the training data and secondly since it is tested against a small amount of data, it has high chances of overfitting. To overcome this, there is a technique called cross validation. The most common form of cross validation, and the one we will be using, is called k-fold cross validation. ‘Fold’ refers to each different iteration that we train our model on, and ‘k’ just refers to the number of folds. In the diagram above, we have illustrated k-fold validation where k is 5.\n",
    "\n",
    "\n",
    "__eg__\n",
    "\n",
    "When classes are imbalanced, accuracy is not a suitable measure.\n",
    "The target variable was highly unbalanced with the positive class (heavy drinkers) making up less than 5% of the population. For this reason, recall was chosen as the performance metric rather than accuracy. That is, performance should be weighted towards identifying heavy drinkers even if it means incorrectly classifying some normal drinkers as heavy drinkers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q What is sparse soluttion? How Logistics Regression can be used to perform feature selection?__\n",
    "\n",
    "As gamma increases, more variables' coefficients go to 0. Once a variable has a 0 coefficient, it has no impact on the model anymore. So, as gamma increases, the model uses fewer and fewer variables. This is what we mean by a sparse solution - it only uses a few variables in the dataset.\n",
    "\n",
    "A logistic regression with l1 penalty yields sparse models, and can thus be used to perform feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q Does regularization in logistic regression always results in better fit and better generalization?__\n",
    "\n",
    "Regularization does NOT improve the performance on the training set that the algorithm used to learn the model parameters (feature weights). However, it can improve the generalization performance, i.e., the performance on new, unseen data, which is exactly what we want.\n",
    "\n",
    "In intuitive terms, we can think of regularization as a penalty against complexity. Increasing the regularization strength penalizes \"large\" weight coefficients -- our goal is to prevent that our model picks up \"peculiarities,\" \"noise,\" or \"imagines a pattern where there is none.\"\n",
    "\n",
    "\n",
    "Again, we don't want the model to memorize the training dataset, we want a model that generalizes well to new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q What is the difference between Logistics Regression and SVM?__\n",
    "\n",
    "- SVM is non-parametric, is a hard classifier but LR is a parametric/probabilistic one.\n",
    "\n",
    "- Linear SVM (without using any kernal fuctions) perform usually as same as Logistic Regression as both are structurally similar differing only their loss function. (hinge for SVM and logistics for LR)\n",
    "\n",
    "- LR produces probabilistic values while SVM produces 1 or 0.SVM may perform worse than LR when the dataset is small,\n",
    "\n",
    "The logistic regression can only separate linearly separable classes where as SVM (with the kernel trick) can find any arbitrarily shaped decision boundary which means better generalization. This means that SVM will usually do better separating your classes (at least on your training set) but is more prone to over-fitting.\n",
    "\n",
    "Logistics regression is also a simpler model with fewer hyper-parameters to tune (zero if you're not using regularization) making it easier to implement.\n",
    "\n",
    "\n",
    "Logistic regression outputs a probability of being in the positive class (you still need to choose a threshold to make it a classifer), SVM just outputs the classes. SVM can give you probabilies via Platt scaling but this can be very slow.\n",
    "\n",
    "\n",
    "\n",
    "To Sum up , use SVM when you have large dataset and large number of feature vectors and there is a clear decision boundary in the dataset. Otherwise use Logistic Regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q What is Pearson’s chi-square test of association/independence? How it is useful in feature selection?__\n",
    "\n",
    "Chi-square test is used for categorical features in a dataset. In practice, we calculate Chi-square between each feature and the target and select the desired number of features with best Chi-square scores.\n",
    "\n",
    "- Chi square test is akin to correlateion \n",
    "- used for testing relationships between categorical variables, categorical response and categorical predictor\n",
    "- The null hypothesis of the Chi-Square test is that no relationship exists on the categorical variables in the population i.e they are independent. \n",
    "- Independent when p > 0.05 and Dependent when p < 0.05, a higher the Chi-Square value the feature is more dependent\n",
    "- Chi-square Test can also be used for feature selection\n",
    "- ANOVA - continuous response and categorical predictor, ANOVA can also be used for feature selection\n",
    "- Chi-Square is sensitive to small frequencies in cells of tables. Generally, when the expected value in a cell of a table is less than 5, chi-square can lead to errors in conclusions.\n",
    "- the other chi square test is goodness of fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original feature number: 4\n",
      "Reduced feature number: 2\n",
      "(array([ 10.28712871,   5.02267003, 133.06854839,  74.27906977]), array([5.83684799e-03, 8.11598175e-02, 1.27213107e-29, 7.42172639e-17]))\n"
     ]
    }
   ],
   "source": [
    "# Chi Square demo\n",
    "from sklearn.datasets import load_iris \n",
    "from sklearn.feature_selection import SelectKBest ,chi2\n",
    "iris_dataset = load_iris() \n",
    "  \n",
    "X = iris_dataset.data \n",
    "y = iris_dataset.target \n",
    "X = X.astype(int) \n",
    "  \n",
    "# Two features with highest chi-squared statistics are selected \n",
    "chi2_features = SelectKBest(chi2, k = 2) \n",
    "X_kbest_features = chi2_features.fit_transform(X, y) \n",
    "  \n",
    "# Reduced features \n",
    "print('Original feature number:', X.shape[1]) \n",
    "print('Reduced feature number:', X_kbest_features.shape[1]) \n",
    "chi_scores = chi2(X,y) # first array rep. chi sq value, 2nd array rep. p-value\n",
    "print(chi_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q What is Logistics Loss / Log Loss or Cross Entropy Loss?__\n",
    "\n",
    "Cross entropy measures the divergence between two probability distribution, if the cross entropy is large, which means that the difference between two distribution is large, while if the cross entropy is small, which means that two distribution is similar to each other.\n",
    "\n",
    "Log Loss uses negative log to provide an easy metric for comparison. It takes this approach because the positive log of numbers < 1 returns negative values, which is confusing to work with when comparing the performance of two models\n",
    "\n",
    "It is the measure of performance of classifier model where the prediction input is a proabability value. The goal is to minimize this value since it is a loss.\n",
    "\n",
    "- log loss = 0 , is considered a perfect model\n",
    "- the log loss is only defined for two or more labels\n",
    "\n",
    "__log loss vs Cross Entropy__\n",
    "\n",
    "log loss and cross entropy are slightly different depending upon the context. In machine learning, when calculating error rates between 0 and 1, the resolve to same thing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q. Why is accuracy not a good measure for classification problems?__\n",
    "\n",
    "Accuracy is not a good measure for classification problems because it gives equal importance to both false positives and false negatives. However, this may not be the case in most business problems. \n",
    "\n",
    "Accuracy gives equal importance to both cases and cannot differentiate between them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q. Why can’t we use Mean Square Error (MSE) as a cost function for logistic regression?__\n",
    "\n",
    "logistic regression uses sigmoid function and perform a non-linear transformation to obtain the probabilities. Squaring this non-linear transformation will lead to non-convexity with local minimums. Finding the global minimum in such cases using gradient descent is not possible. Due to this reason, MSE is not suitable for logistic regression. \n",
    "\n",
    "Cross-entropy or log loss is used as a cost function for logistic regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q What is the difference between “L1” and “L2” regularization?__\n",
    "\n",
    "\n",
    "Regularization refers to methods used to modify an objective function in order to reduce model overfitting.\n",
    "\n",
    "L1 and L2 regularization refer to methods of calculating the length of a vector of model parameters (called the vector norm) in order that this length can be minimized as part of fitting the model.\n",
    "\n",
    "L1 or the L1-norm is calculated as the sum of the absolute vector values. An example use of this form of regularization is used in Lasso Regression.\n",
    "L2 or the L2-norm is calculated as the sum of the squared vector values. An example use of this form of regularization is used in Ridge Regression.\n",
    "The ElasticNet Regression algorithm uses a combination of both L1 and L2 regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Logistic regression is a supervised machine learning classifier that extracts real-valued features from the input, multiplies each by a weight, sums them, and passes the sum through a sigmoid function to generate a probability. A threshold is used to make a decision.\n",
    "- Logistic regression can be used with two classes (e.g., positive and negative sentiment) or with multiple classes (multinomial logistic regression, for example for n-ary text classification, part-of-speech labeling, etc.).\n",
    "- Multinomial logistic regression uses the softmax function to compute probabilities.\n",
    "- The weights (vector w and bias b) are learned from a labeled training set via a loss function, such as the cross-entropy loss, that must be minimized.\n",
    "- Minimizing this loss function is a convex optimization problem, and iterative algorithms like gradient descent are used to find the optimal weights.\n",
    "- Regularization is used to avoid overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
