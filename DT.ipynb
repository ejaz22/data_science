
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Supervised learning algorithm, logical model, Non - parameteric\n",
    "- Classification and Regression Trees (CART) is refer to the Decision Tree algorithm that can be learned for classification or regression problems. It uses the Gini method to create split points.\n",
    "- decision trees are high variance algorithm, meaning that different splits in the training data can lead to very different trees. \n",
    "\n",
    "The main objective of the decision tree is to split data in such a way that each element in one group belongs to the same category. The splitting up of data is based on some measures that partition data into the best possible manner.The most popular measures are:\n",
    "\n",
    "- Gini index\n",
    "- Information gain\n",
    "- chi square\n",
    "\n",
    "__Gini Index__\n",
    "\n",
    "- The Gini index is the name of the cost function used to evaluate splits in the dataset.\n",
    "\n",
    "- Gini Index is a metric to measure how often a randomly chosen element would be incorrectly identified. It means an attribute with lower gini index should be preferred.\n",
    "\n",
    "- The degree of the Gini index falls between 0 and 1, where 0 denotes that all the elements belong to a certain class and 1 denotes that the elements are randomly distributed across various classes. When the value of Gini is equal to 0, the node is considered pure and no further split is done. \n",
    "\n",
    "__Information Gain__\n",
    "\n",
    "information gain is derived from entropy. Entropy is a way of measuring the amount of impurity in a given set of data\n",
    "\n",
    "Information gain is used to determine which feature gives us the maximum information about a class. \n",
    "\n",
    "High entropy means that we have a collection of different classes and a low entropy means that we have predominantly one class, therefore, we keen on splitting the node in a way that decreases the entropy. \n",
    "\n",
    "Information gain is the decrease in entropy. Information gain computes the difference between entropy before split and average entropy after split of the dataset based on given attribute values. ID3 (Iterative Dichotomiser) decision tree algorithm uses information gain.\n",
    "\n",
    "IG = information before splitting (parent) — information after splitting (children)\n",
    "\n",
    "Information gain is biased for the attribute with many outcomes. It means it prefers the attribute with a large number of distinct values. For instance, consider an attribute with a unique identifier such as customer_ID has zero info(D) because of pure partition. This maximizes the information gain and creates useless partitioning.\n",
    "\n",
    "\n",
    "__Assumption__\n",
    "\n",
    "- In the beginning, the whole training set is considered at the root.\n",
    "- The root node (the first decision node) partitions the data using the feature that provides the most information gain.\n",
    "- Feature values are preferred to be categorical. If values are continuous then they are discretized prior to building the model.\n",
    "- Records are distributed recursively on the basis of attribute values.\n",
    "- Order to placing attributes as root or internal node of the tree is done by using some statistical approach.\n",
    "\n",
    "__How it works__\n",
    "-  It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes.\n",
    "Pruning: When we remove sub-nodes of a decision node, this process is called pruning. You can say opposite process of splitting.\n",
    "\n",
    "__How to choose and optimal max_depth for the tree?__\n",
    "\n",
    "- a pruned tree that is less complex, explainable, and easy to understand.\n",
    "\n",
    "\n",
    "\n",
    "- The core algorithm for building decision trees called ID3.  ID3 uses Entropy and Information Gain to construct a decision tree. ID3 algorithm uses entropy to calculate the homogeneity of a sample.\n",
    "\n",
    "If the sample is completely homogeneous the entropy is zero and if the sample is an equally divided it has entropy of one.\n",
    "\n",
    "Not fit for continuous variables: While working with continuous numerical variables, decision tree looses information when it categorizes variables in different categories.\n",
    "\n",
    "It requires fewer data preprocessing from the user, for example, there is no need to normalize columns.\n",
    "\n",
    "The small variation(or variance) in data can result in the different decision tree. This can be reduced by bagging and boosting algorithms.\n",
    "\n",
    "Decision trees are biased with imbalance dataset, so it is recommended that balance out the dataset before creating the decision tree.\n",
    "\n",
    "criterion: It is used to measure the quality of a split in the decision tree classification. By default, it is ‘gini’; it also supports ‘entropy’.\n",
    "\n",
    "max_depth: This is used to add maximum depth to the decision tree after the tree is expanded.\n",
    "min_samples_leaf: This parameter is used to add the minimum number of samples required to be present at a leaf node.\n",
    "\n",
    "Visualizing decsion tree: Using graphviz we can visualize the tree.\n",
    "export_graphviz function converts decision tree classifier into dot file and pydotplus convert this dot file to png or displayable form on Jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.DataFrame({\"toothed\":[\"True\",\"True\",\"True\",\"False\",\"True\",\"True\",\"True\",\"True\",\"True\",\"False\"],\n",
    "                     \"hair\":[\"True\",\"True\",\"False\",\"True\",\"True\",\"True\",\"False\",\"False\",\"True\",\"False\"],\n",
    "                     \"breathes\":[\"True\",\"True\",\"True\",\"True\",\"True\",\"True\",\"False\",\"True\",\"True\",\"True\"],\n",
    "                     \"legs\":[\"True\",\"True\",\"False\",\"True\",\"True\",\"True\",\"False\",\"False\",\"True\",\"True\"],\n",
    "                     \"species\":[\"Mammal\",\"Mammal\",\"Reptile\",\"Mammal\",\"Mammal\",\"Mammal\",\"Reptile\",\"Reptile\",\"Mammal\",\"Reptile\"]}, \n",
    "                    columns=[\"toothed\",\"hair\",\"breathes\",\"legs\",\"species\"])\n",
    "\n",
    "features = data[[\"toothed\",\"hair\",\"breathes\",\"legs\"]]\n",
    "target = data[\"species\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toothed</th>\n",
       "      <th>hair</th>\n",
       "      <th>breathes</th>\n",
       "      <th>legs</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Mammal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Mammal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Reptile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Mammal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Mammal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Mammal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Reptile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Reptile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Mammal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Reptile</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  toothed   hair breathes   legs  species\n",
       "0    True   True     True   True   Mammal\n",
       "1    True   True     True   True   Mammal\n",
       "2    True  False     True  False  Reptile\n",
       "3   False   True     True   True   Mammal\n",
       "4    True   True     True   True   Mammal\n",
       "5    True   True     True   True   Mammal\n",
       "6    True  False    False  False  Reptile\n",
       "7    True  False     True  False  Reptile\n",
       "8    True   True     True   True   Mammal\n",
       "9   False  False     True   True  Reptile"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forests are an example of an ensemble learner built on decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q When would you use Decision tree over Random Forest?__\n",
    "\n",
    "- When the data is small and  is more non-parametric in nature and we are not worried about accuracy on future datasets.\n",
    "- Easy to compute and explain why a particular variable is having higher importance\n",
    "- If the goal is exploratory analysis, we should prefer a single DT , as to understand the data relationship in a tree hierarchy structure.\n",
    "- The tree can be visualized and hence, for non-technical users, it is easier to explain model implementation\n",
    "- DTs are prone to overfitting. Setting a max depth solves overfitting but introduces bias.\n",
    "\n",
    "Random forest should be preferred if:\n",
    "- If the goal is better predictions, we should prefer RF, to reduce the variance. when accuracy is prioritised over explainability\n",
    "- when data has high bias, employing bagging and sampling techniques correctly will reduce over fitting\n",
    "\n",
    "If we need to analyse a bigger dataset, with a lot of features with a higher demand for accuracy a Random Forest would be a better choice.\n",
    "By bootstraping over our dataset, a Random Forest would be more acurate and generalistic model. However it is importante to note that even a\n",
    "Random Forest would have trouble with outliers and rare events on our dataset. Depending of our problem another model such as neural network would be more\n",
    "appropriate.\n",
    "\n",
    "One single DT would lead to over-fit model if the dataset is huge (i.e. one person's POV)\n",
    "However, if we have a voting mechanism and ask different individuals/trees to interpret the data then we would be able to cover the patterns in a much meticulous way. This is with the case of RF.\n",
    "\n",
    "Decision trees work well when the data is less complex and the splits are easily determinable. Random forests would be helpful in cases where the data is comparatively large and number of features huge. Also, random forests can handle well, noisy and missing data as compared to decision trees.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q What is difference between Gini Impurity and Entropy in Decision Tree?__\n",
    "\n",
    "There are three commonly used impurity measures used in binary decision trees: \n",
    "\n",
    "- Entropy (a way to measure impurity)\n",
    "- Gini index (a criteria to minimize probability of misclassification)\n",
    "- and Classification Error\n",
    "\n",
    "- Both Gini Impurity and Entropy are criteria to split a node in a decision tree. \n",
    "- Gini is intended for continuous attributes, and Entropy for attributes that occur in classes.\n",
    "- Gini will tend to find the largest class, and entropy tends to find groups of classes that make up ~50% of the data.\n",
    "- Gini to minimize misclassification and Entropy for exploratory analysis.\n",
    "- Because the ensemble model(RF) is quite robust and resistant to noise from the individual decision trees, we typically don't need to prune the random forest, and the only parameter we care about is the number of trees i.e k\n",
    "\n",
    "\n",
    "Most of the times, performance of a model won’t change whether you use Gini or Entropy.In terms if computation, Entropy takes more time as it includes LOG function.\n",
    "\n",
    "In classification trees, the Gini Index is used to compute the impurity of a data partition. So Assume the data partition D consisiting of 4 classes each with equal probability. Then the Gini Index (Gini Impurity) will be: \n",
    "Gini(D) = 1 - (0.25^2 + 0.25^2 + 0.25^2 + 0.25^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NOTE__ \n",
    "\n",
    "- The entropy is 0 if all samples of a node belong to the same class, and the entropy is maximal if we have a uniform class distribution. In other words, the entropy of a node (consist of single class) is zero because the probability is 1 and log (1) = 0. Entropy reaches maximum value when all classes in the node have equal probability.\n",
    "\n",
    "- Gini index is an intermediate measure between entropy and the classification error.\n",
    "\n",
    "- underfitting (high bias)\n",
    "\n",
    "- In order to avoid overfitting, it is necessary to use additional techniques (e.g. cross-validation, regularization, early stopping, pruning, or Bayesian priors).\n",
    "\n",
    "- Regularization is a way of finding a good bias-variance tradeoff by tuning the complexity of the model. It is a very useful method to handle collinearity (high correlation among features), filter out noise from data, and eventually prevent overfitting. The concept behind regularization is to introduce additional information (bias) to penalize extreme parameter weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in SVMs, our optimization objective is to maximize the margin. The primary reason for having decision boundaries with large margins is that they tend to have a lower generalization error whereas models with small margins are more prone to overfitting \n",
    "\n",
    "The reason for introducing the slack variable ξ is that the linear constraints need to be relaxed for nonlinearly separable data to allow convergence of the optimization in the presence of misclassifications under the appropriate cost penalization.\n",
    "\n",
    "With the variable C, we can penalize for misclassification. Large values of C correspond to large error penalties while we are less strict about misclassification errors if we choose smaller values for C. We can then use the parameter C to control the width of the margin and therefore tune the bias-variance trade-off as shown in the picture below:\n",
    "\n",
    "The basic morale of kernel methods is to deal with linearly inseparable data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The principle behind Maximum Entropy [4] is that the correct distribution is the one that maximizes the Entropy / uncertainty and still meets the constraints which are set by the ‘evidence’."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q When would you prefer decision tree?__\n",
    "\n",
    "Advantages of using decision tree are that it does not require much of data preprocessing and it does not require any assumptions of distribution of data. This algorithm is very useful to identify the hidden pattern in the dataset.\n",
    "When the data is simple with less features, decision tree might be useful, Otherwise Random Forest will give better predictions.\n",
    "\n",
    "Random forests can use random subset of features and/or samples for it's trees whereas decision tree do not.\n",
    " We prefer the Decision Tree to the Random Forest when the interpretability is more important than the accuracy.\n",
    "We can easily visualize our Decision Tree and understand the decision-sequence for prediction of this machine learning algorithm when we want to describe model for business users. With Random Forest we can visualize one, two or all trees in forest, but we can't understand the summary decision-sequence for whole forest.\n",
    "\n",
    "- You want visuals => DT\n",
    "- You want POWER => RF\n",
    "\n",
    "Therefore a decision tree would be better if a simple and fast model already meets our current EDA and prediction demands. However, it is model that is very prone to overfitting. If we need to analyse a bigger dataset, with a lot of features with a higher demand for accuracy a Random Forest would be a better choice.\n",
    "By bootstraping over our dataset, a Random Forest would be more accurate and generalistic model. However it is important to note that even a Random Forest would have trouble with outliers and rare events on our dataset. Depending of our problem another model such as neural network would be more appropriate.\n",
    "\n",
    "DecisionTrees are preferred over RandomForests in few cases:-\n",
    "- 1. When you have less training data. As we know that Random forest is the ensembling of Decision trees having the less data and trying to implement the Random Forest can cause overfitting as it tries to match each and every input in input dataset\n",
    "- 2. When the computational power is limited.\n",
    "- 3. When we want the model to be simple and explainable even to the business users.\n",
    "\n",
    "Decision tree advantages:\n",
    "- 1.) Easy to understand and easy to implement.\n",
    "- 2.) Runs fast.\n",
    "- 3.) Scales well with large datasets.\n",
    "- 4.) Works on numerical and categorical data.\n",
    "- 5.) Algorithm workings can be observed, so work can be reproduced.\n",
    "\n",
    "Decision tree disadvantages:\n",
    "- 1.) Prone to overfitting. Setting a max depth solves overfitting but introduces bias.\n",
    "- 2.) Running decision tree algorithms in a reasonable timespan requires greedy algorithms, which produces local optimum instead of global optimum.\n",
    "\n",
    "Decision tree is better than random forest if speed is critical and accuracy can be traded off.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q Scaling Vs Normalization - What is the difference?__\n",
    "\n",
    "We need scaling mainly for algorithms which internally uses some distance measure technique (say Euclidian Distance. Whereas, Normalization is needed, when comparing populations/phenomena of different size but with the same origin.\n",
    "Scaling just changes the range of your data. This means that you're transforming your data so that it fits within a specific scale, like 0-100 or 0-1. You want to scale data when you're using methods based on measures of how far apart data points, like SVM or KNN. With these algorithms, a change of \"1\" in any numeric feature is given the same importance.\n",
    "\n",
    "For example, you might be looking at the prices of some products in both Yen and US Dollars. One US Dollar is worth about 100 Yen, but if you don't scale your prices methods like SVM or KNN will consider a difference in price of 1 Yen as important as a difference of 1 US Dollar! This clearly doesn't fit with our intuitions of the world. With currency, you can convert between currencies. But what about if you're looking at something like height and weight? It's not entirely clear how many pounds should equal one inch (or how many kilograms should equal one meter).\n",
    "By scaling your variables, you can help compare different variables on equal footing. To help solidify what scaling looks like, let's look at a made-up example. (Don't worry, we'll work with real data in just a second, this is just to help illustrate my point.)\n",
    "\n",
    "In general, you'll only want to normalize your data if you're going to be using a machine learning or statistics technique that assumes your data is normally distributed. Some examples of these include t-tests, ANOVAs, linear regression, linear discriminant analysis (LDA) and Gaussian naive Bayes. (Pro tip: any method with \"Gaussian\" in the name probably assumes normality.)\n",
    "The method were using to normalize here is called the Box-Cox Transformation. Let's take a quick peek at what normalizing some data looks like:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "__Q What Is The Difference Between PCA and PLS (Principal Component Analysis VS. Partial Least Squares)?__\n",
    "\n",
    "__PCA__\n",
    "PCA tries to explain the variance-covariance structure of a data set. Aim is to increase the variance of the features itself, like the loss of information is greatly reduced. PCA is a Dimensionality Reduction algorithm. Both PLS and PCA are used for dimension reduction.\n",
    "\n",
    "__PLS__\n",
    "Partial Least Squares, use the annotated label to maximize inter-class variance. Principal components are pairwise orthogonal. Principal components are focus on maximize correlation.\n",
    "The main difference is that the PCA is unsupervised method and PLS is supervised method.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q You are given a data set for classification, which model would you use and why: Logistic Regression, SVM or Neural Networks?__\n",
    "\n",
    "In general, it depends on the kind of data and amount of samples x features. \n",
    "\n",
    "For text classification/categorization:  I would recommend to use naive Bayes or linear SVM.\n",
    "\n",
    "For datasets with numerical attributes: I would suggest linear SVM, neural networks or logistic regression if the amount of features is much greater than the number of samples.\n",
    "\n",
    "On the other hand, I would recommend neural networks or SVM with RBF or polynomial kernel if the amount of samples is not too large and greater than the number of features.\n",
    "\n",
    "Otherwise, if the number of samples is huge I would suggest to use neural networks or linear SVM, and so on. ANN requires large data-set for training .\n",
    "\n",
    "-----------------------------------\n",
    "\n",
    "SVMs are really good when you have a high dimensionality dataset and you don't have a lot of data.as it can handle high dimensional data or a data-set with high number of features. SVM is better for those situations where data-set is not too large. SVM without kernel is also a linear classifier.\n",
    "\n",
    "Logistic Regression is best when you have linear/Binary classification problems. LR is a very good all-purpose algorithm, if you need probabilities or you have a lot of data LR is usually good.\n",
    "\n",
    "- reduced number of features => LR\n",
    "- a lot of features but not a lot of data => SVM\n",
    "- a lot of features and a lot of data => NN\n",
    "\n",
    "--------------------------------------------------------------------\n",
    "Linear SVM (with liner kernel) and LR are classification methods with linear decision boundaries\n",
    "LR produces probabilistic values while SVM produces 1 or 0\n",
    "SVMs are great for relatively small data sets with fewer outliers\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
