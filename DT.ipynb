{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree is a Supervised learning algorithm, logical model, non-parameteric. DTs are high variance algorithm, meaning that different splits in the training data can lead to very different trees. The main objective of the decision tree is to split data in such a way that each element in one group belongs to the same category. The splitting up of data is based on some measures that partition data into the best possible manner.The most popular measures are:\n",
    "\n",
    "- Gini index\n",
    "- Information gain\n",
    "- chi square\n",
    "\n",
    "__Gini Index__\n",
    "\n",
    "- The Gini index is the name of the cost function used to evaluate splits in the dataset.\n",
    "\n",
    "- Gini Index is a metric to measure how often a randomly chosen element would be incorrectly identified. It means an attribute with lower gini index should be preferred.\n",
    "\n",
    "- The degree of the Gini index falls between 0 and 1, where 0 denotes that all the elements belong to a certain class and 1 denotes that the elements are randomly distributed across various classes. When the value of Gini is equal to 0, the node is considered pure and no further split is done. \n",
    "\n",
    "__Entropy__\n",
    "\n",
    "Entropy is a measure of node impurity. For a binary class (a,b), the formula to calculate it is shown below. Entropy is maximum at p = 0.5. For p(X=a)=0.5 or p(X=b)=0.5 means, a new observation has a 50%-50% chance of getting classified in either classes. The entropy is minimum when the probability is 0 or 1.\n",
    "\n",
    "__Information Gain (IG)__\n",
    "\n",
    "- Information gain is used to determine which feature gives us the maximum information about a class. IG is derived from entropy. Entropy is a way of measuring the amount of impurity in a given dataset\n",
    "\n",
    "- High entropy means that we have a collection of different classes and a low entropy means that we have predominantly one class, therefore, we keen on splitting the node in a way that decreases the entropy. \n",
    "\n",
    "- Information gain is the decrease in entropy. IG computes the difference between entropy before split and average entropy after split of the dataset based on given attribute values. ID3 (Iterative Dichotomiser) decision tree algorithm uses information gain.\n",
    "\n",
    "- IG = information before splitting (parent) — information after splitting (children)\n",
    "\n",
    "- Information gain is biased for the attribute with many outcomes. It means it prefers the attribute with a large number of distinct values. For instance, consider an attribute with a unique identifier such as customer_ID has zero info(D) because of pure partition. This maximizes the information gain and creates useless partitioning.\n",
    "\n",
    "\n",
    "__Assumption__\n",
    "\n",
    "- In the beginning, the whole training set is considered at the root.\n",
    "- The root node (the first decision node) partitions the data using the feature that provides the most information gain.\n",
    "- Feature values are preferred to be categorical. If values are continuous then they are discretized prior to building the model.\n",
    "- Records are distributed recursively on the basis of attribute values.\n",
    "- Order to placing attributes as root or internal node of the tree is done by using some statistical approach.\n",
    "\n",
    "__How it works__\n",
    "-  It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes.\n",
    "\n",
    "__Pruning__\n",
    "- When we remove sub-nodes of a decision node, this process is called pruning. You can say opposite process of splitting.\n",
    "- To achieve a trade-of between prediction accuracy and complexity\n",
    "\n",
    "__How to choose and optimal max_depth for the tree?__\n",
    "\n",
    "- a pruned tree that is less complex, explainable, and easy to understand.\n",
    "\n",
    "- The core algorithm for building decision trees called ID3.  ID3 uses Entropy and Information Gain to construct a decision tree. ID3 algorithm uses entropy to calculate the homogeneity of a sample.\n",
    "\n",
    "- Not fit for continuous variables: While working with continuous numerical variables, decision tree looses information when it categorizes variables in different categories.\n",
    "\n",
    "- It requires fewer data preprocessing from the user, for example, there is no need to normalize columns.\n",
    "\n",
    "- The small variation(or variance) in data can result in the different decision tree. This can be reduced by bagging and boosting algorithms.\n",
    "\n",
    "- Decision trees are biased with imbalance dataset, so it is recommended that balance out the dataset before creating the decision tree.\n",
    "\n",
    "__Other Parameters__\n",
    "\n",
    "criterion: It is used to measure the quality of a split in the decision tree classification. By default, it is ‘gini’; it also supports ‘entropy’.\n",
    "\n",
    "max_depth: This is used to add maximum depth to the decision tree after the tree is expanded.\n",
    "\n",
    "min_samples_leaf: This parameter is used to add the minimum number of samples required to be present at a leaf node.\n",
    "\n",
    "__Visualizing decsion tree:__ Using graphviz we can visualize the tree. export_graphviz function converts decision tree classifier into dot file and pydotplus convert this dot file to png or displayable form on Jupyter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q When would you use Decision tree over Random Forest?__\n",
    "\n",
    "- When the data is small and  is more non-parametric in nature and we are not worried about accuracy on future datasets.\n",
    "- Easy to compute and explain why a particular variable is having higher importance\n",
    "- If the goal is exploratory analysis, we should prefer a single DT , as to understand the data relationship in a tree hierarchy structure.\n",
    "- The tree can be visualized and hence, for non-technical users, it is easier to explain model implementation\n",
    "- DTs are prone to overfitting. Setting a max depth solves overfitting but introduces bias.\n",
    "\n",
    "Random forest should be preferred if:\n",
    "- If the goal is better predictions, we should prefer RF, to reduce the variance. when accuracy is prioritised over explainability\n",
    "- when data has high bias, employing bagging and sampling techniques correctly will reduce over fitting\n",
    "\n",
    "If we need to analyse a bigger dataset, with a lot of features with a higher demand for accuracy a Random Forest would be a better choice.\n",
    "By bootstraping over our dataset, a Random Forest would be more acurate and generalistic model. However it is importante to note that even a\n",
    "Random Forest would have trouble with outliers and rare events on our dataset. Depending of our problem another model such as neural network would be more\n",
    "appropriate.\n",
    "\n",
    "One single DT would lead to over-fit model if the dataset is huge (i.e. one person's POV)\n",
    "However, if we have a voting mechanism and ask different individuals/trees to interpret the data then we would be able to cover the patterns in a much meticulous way. This is with the case of RF.\n",
    "\n",
    "Decision trees work well when the data is less complex and the splits are easily determinable. Random forests would be helpful in cases where the data is comparatively large and number of features huge. Also, random forests can handle well, noisy and missing data as compared to decision trees.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q What is difference between Gini Impurity and Entropy in Decision Tree?__\n",
    "\n",
    "There are three commonly used impurity measures used in binary decision trees: \n",
    "\n",
    "- Entropy (a way to measure impurity)\n",
    "- Gini index (a criteria to minimize probability of misclassification)\n",
    "- and Classification Error\n",
    "\n",
    "- Both Gini Impurity and Entropy are criteria to split a node in a decision tree. \n",
    "- Gini is intended for continuous attributes, and Entropy for attributes that occur in classes.\n",
    "- Gini will tend to find the largest class, and entropy tends to find groups of classes that make up ~50% of the data.\n",
    "- Gini to minimize misclassification and Entropy for exploratory analysis.\n",
    "- Because the ensemble model(RF) is quite robust and resistant to noise from the individual decision trees, we typically don't need to prune the random forest, and the only parameter we care about is the number of trees i.e k\n",
    "\n",
    "\n",
    "Most of the times, performance of a model won’t change whether you use Gini or Entropy.In terms of computation, Entropy takes more time as it includes Log function.\n",
    "\n",
    "In classification trees, the Gini Index is used to compute the impurity of a data partition. So Assume the data partition D consisiting of 4 classes each with equal probability. Then the Gini Index (Gini Impurity) will be: \n",
    "Gini(D) = 1 - (0.25^2 + 0.25^2 + 0.25^2 + 0.25^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NOTE__ \n",
    "\n",
    "- If the sample is completely homogeneous the entropy is zero and if the sample is an equally divided it has entropy of one.\n",
    "- The entropy is 0 if all samples of a node belong to the same class, and the entropy is maximal if we have a uniform class distribution. In other words, the entropy of a node (consist of single class) is zero because the probability is 1 and log (1) = 0. Entropy reaches maximum value when all classes in the node have equal probability.\n",
    "\n",
    "- Gini index is an intermediate measure between entropy and the classification error.\n",
    "\n",
    "- underfitting (high bias)\n",
    "\n",
    "- In order to avoid overfitting, it is necessary to use additional techniques (e.g. cross-validation, regularization, early stopping, pruning, or Bayesian priors).\n",
    "\n",
    "- Regularization is a way of finding a good bias-variance tradeoff by tuning the complexity of the model. It is a very useful method to handle collinearity (high correlation among features), filter out noise from data, and eventually prevent overfitting. The concept behind regularization is to introduce additional information (bias) to penalize extreme parameter weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in SVMs, our optimization objective is to maximize the margin. The primary reason for having decision boundaries with large margins is that they tend to have a lower generalization error whereas models with small margins are more prone to overfitting \n",
    "\n",
    "The reason for introducing the slack variable ξ is that the linear constraints need to be relaxed for nonlinearly separable data to allow convergence of the optimization in the presence of misclassifications under the appropriate cost penalization.\n",
    "\n",
    "With the variable C, we can penalize for misclassification. Large values of C correspond to large error penalties while we are less strict about misclassification errors if we choose smaller values for C. We can then use the parameter C to control the width of the margin and therefore tune the bias-variance trade-off as shown in the picture below:\n",
    "\n",
    "The basic morale of kernel methods is to deal with linearly inseparable data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The principle behind Maximum Entropy [4] is that the correct distribution is the one that maximizes the Entropy / uncertainty and still meets the constraints which are set by the ‘evidence’."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q When would you prefer decision tree?__\n",
    "\n",
    "DTs does not require much of data preprocessing and it does not require any assumptions of distribution of data. This algorithm is very useful to identify the hidden pattern in the dataset. When the data is simple with less features, decision tree might be useful, Otherwise Random Forest will give better predictions.\n",
    "\n",
    "Random forests can use random subset of features and/or samples for it's trees whereas decision tree do not. We prefer the Decision Tree to the Random Forest when the interpretability is more important than the accuracy. We can easily visualize our Decision Tree and understand the decision-sequence for prediction of this machine learning algorithm when we want to describe model for business users. With Random Forest we can visualize one, two or all trees in forest, but we can't understand the summary decision-sequence for whole forest.\n",
    "\n",
    "- You want visuals => DT\n",
    "- You want POWER => RF\n",
    "\n",
    "Therefore a decision tree would be better if a simple and fast model already meets our current EDA and prediction demands. However, it is model that is very prone to overfitting. If we need to analyse a bigger dataset, with a lot of features with a higher demand for accuracy a Random Forest would be a better choice.\n",
    "By bootstraping over our dataset, a Random Forest would be more accurate and generalistic model. However it is important to note that even a Random Forest would have trouble with outliers and rare events on our dataset. Depending of our problem another model such as neural network would be more appropriate.\n",
    "\n",
    "DecisionTrees are preferred over RandomForests in few cases:-\n",
    "- 1. When you have less training data. As we know that Random forest is the ensembling of Decision trees having the less data and trying to implement the Random Forest can cause overfitting as it tries to match each and every input in input dataset\n",
    "- 2. When the computational power is limited.\n",
    "- 3. When we want the model to be simple and explainable even to the business users.\n",
    "\n",
    "Decision tree advantages:\n",
    "- 1.) Easy to understand and easy to implement.\n",
    "- 2.) Runs fast.\n",
    "- 3.) Scales well with large datasets.\n",
    "- 4.) Works on numerical and categorical data.\n",
    "- 5.) Algorithm workings can be observed, so work can be reproduced.\n",
    "\n",
    "Decision tree disadvantages:\n",
    "- 1.) Prone to overfitting. Setting a max depth solves overfitting but introduces bias.\n",
    "- 2.) Running decision tree algorithms in a reasonable timespan requires greedy algorithms, which produces local optimum instead of global optimum.\n",
    "\n",
    "Decision tree is better than random forest if speed is critical and accuracy can be traded off.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q You are given a data set for classification, which model would you use and why: Logistic Regression, SVM or Neural Networks?__\n",
    "\n",
    "In general, it depends on the kind of data and amount of samples x features. \n",
    "\n",
    "For text classification/categorization:  I would recommend to use naive Bayes or linear SVM.\n",
    "\n",
    "For datasets with numerical attributes: I would suggest linear SVM, neural networks or logistic regression if the amount of features is much greater than the number of samples.\n",
    "\n",
    "On the other hand, I would recommend neural networks or SVM with RBF or polynomial kernel if the amount of samples is not too large and greater than the number of features.\n",
    "\n",
    "Otherwise, if the number of samples is huge I would suggest to use neural networks or linear SVM, and so on. ANN requires large data-set for training .\n",
    "\n",
    "SVMs are really good when you have a high dimensionality dataset and you don't have a lot of data.as it can handle high dimensional data or a data-set with high number of features. SVM is better for those situations where data-set is not too large. SVM without kernel is also a linear classifier.\n",
    "\n",
    "Logistic Regression is best when you have linear/Binary classification problems. LR is a very good all-purpose algorithm, if you need probabilities or you have a lot of data LR is usually good.\n",
    "\n",
    "- reduced number of features => LR\n",
    "- a lot of features but not a lot of data => SVM\n",
    "- a lot of features and a lot of data => NN\n",
    "\n",
    "Linear SVM (with liner kernel) and LR are classification methods with linear decision boundaries\n",
    "LR produces probabilistic values while SVM produces 1 or 0\n",
    "SVMs are great for relatively small data sets with fewer outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q Do you think 50 small decision trees are better than a large one? Why?__\n",
    "\n",
    "Yes!\n",
    "More robust model (ensemble of weak learners that come and make a strong learner)\n",
    "Better to improve a model by taking many small steps than fewer large steps\n",
    "\n",
    "If one tree is erroneous, it can be auto-corrected by the following.\n",
    "Less prone to overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest (RF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DTs work very well especially if they are of small depth. However, DTs with real-world datasets can have large depths. Higher depth DTs are more prone to overfitting and thus lead to higher variance in the model. This shortcoming of DT is explored by the Random Forest model.\n",
    "The Purpose of Random Forest is to improve Prediction Accuracy.\n",
    "\n",
    "```python\n",
    "# Determine how many trees to grow. Using 100 trees in the RF is a good place to start\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators=100)\n",
    "```\n",
    "\n",
    "\n",
    "__stopping parameters while continuing the splits__\n",
    "- a) max_depth: The maximum depth that you allow the tree to grow to. The deeper you allow, the more complex your model will become. Increasing max_depth, training error will always go down (or at least not go up).The deeper the tree, the more splits it has and it captures more information about the data.\n",
    "- b) min_samples_split: min_sample_split is the minimum no. of samples or data points required for a split. For instance, if min_sample_split = 6 and there are 4 sample in the node, then the split will not happen (regardless of entropy).\n",
    "- c) min_sample_leaf: the minimum number of data points required in each leaf to continue the splits.This parameter is similar to min_samples_splits, however, this describe the minimum number of samples of samples at the leafs, the base of the tree.\n",
    "- d) max_leaf_nodes: the maximum number of (supposedly best) leaf nodes.\n",
    "\n",
    "__OUT OF BAG Error__\n",
    "\n",
    "RF technique involves sampling of the input data with replacement (bootstrap sampling). In this sampling, about one thrird of the data is not used for training and can be used to testing.These are called the out of bag samples. Error estimated on these out of bag samples is the out of bag error.\n",
    "The OOB error is often used for assessing the prediction performance of RF. An advantage of the OOB error is that the complete original sample is used both for constructing the RF classifier and for error estimation.\n",
    "\n",
    "__Parameter Tuning__\n",
    "\n",
    "As you increase max_depth you increase variance and decrease bias. On the other hand, as you increase min_samples_leaf you decrease variance and increase bias. So, these parameters will control the level of regularization when growing the trees. In summary, decreasing any of the max* parameters and increasing any of the min* parameters will increase regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q What is Bagging and Boosting?__\n",
    "\n",
    "Bagging and random forests are “bagging” algorithms that aim to reduce the complexity of models that overfit the training data. \n",
    "Bagging (stands for Bootstrap Aggregating) is a way to decrease the variance of your prediction by generating additional data for training from your original dataset using combinations with repetitions to produce multisets of the same cardinality/size as your original data. By increasing the size of your training set you can't improve the model predictive force, but just decrease the variance, narrowly tuning the prediction to expected outcome.\n",
    "In contrast, boosting is an approach to increase the complexity of models that suffer from high bias, that is, models that underfit the training data.\n",
    "\n",
    "__Bagging (Bootstrap AGGregatING)__\n",
    "- Before understand Bagging lets understand the concept of Bootstrap which is nothing but choosing a Random sample with replacement.\n",
    "- parallel ensemble: each model is built independently\n",
    "- aim to decrease variance, not bias\n",
    "- suitable for high variance low bias models (complex models)\n",
    "- an example of a tree based method is random forest, which develop fully grown trees (note that RF modifies the grown procedure to reduce the correlation between trees)\n",
    "\n",
    "\n",
    "\n",
    "Generate n different bootstrap training sample\n",
    "Train Algorithm on each bootstrapped sample separately\n",
    "Average the predictions at the end\n",
    "One of the Key differences is the way how use sample each training set. Bagging allows replacement in bootstrapped sample but Boosting doesn’t.\n",
    "\n",
    "__Boosting__\n",
    "- sequential ensemble: try to add new models that do well where previous models lack\n",
    "- aim to decrease bias, not variance\n",
    "- suitable for low variance high bias models\n",
    "- an example of a tree based method is gradient boosting\n",
    "- The difference from Bagging is that later model is trying to learn the error made by previous one, for example GBM and XGBoost, which eliminate the variance but have overfitting issue\n",
    "- GBDT Algorithms are Xgboost,LightGBM and Gradient Boosting Trees\n",
    "- XGBoost ias also called regularized boosting techninque  which helps to reduce overfitting\n",
    "\n",
    "\n",
    "__Bootstrap Sampling__\n",
    "\n",
    "- Bootstrap sampling means drawing random samples from training set with replacement. if our training set consists of 7 training samples, our bootstrap samples (here: n=7)\n",
    "- it produces multisets of the same cardinality/size as your original data.\n",
    "- eg2. So if i have ABCDE as my values, i can bootstrap with AABCD, etc. I can use values twice, that is the key\n",
    "- It can be used to estimate summary statistics such as the mean or standard deviation.\n",
    "\n",
    "__Stacking__\n",
    "- Bagging and boosting tend to use many homogeneous models and Stacking combines results from heterogenous model types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q What is the difference between Bagging and Random Forest?__\n",
    "\n",
    "Many a time, we fail to ascertain that bagging is not same as random forest. To understand the difference, let's see how bagging works:\n",
    "\n",
    "It creates randomized samples of the data set (just like random forest) and grows trees on a different sample of the original data. The remaining 1/3 of the sample is used to estimate unbiased OOB error.\n",
    "It considers all the features at a node (for splitting).\n",
    "Once the trees are fully grown, it uses averaging or voting to combine the resultant predictions.\n",
    "Aren't you thinking, \"If both the algorithms do same thing, what is the need for random forest? Couldn't we have accomplished our task with bagging?\" NO!\n",
    "\n",
    "The need for random forest surfaced after discovering that the bagging algorithm results in correlated trees when faced with a data set having strong predictors. Unfortunately, averaging several highly correlated trees doesn't lead to a large reduction in variance.\n",
    "\n",
    "But how do correlated trees emerge? Good question! Let's say a data set has a very strong predictor, along with other moderately strong predictors. In bagging, a tree grown every time would consider the very strong predictor at its root node, thereby resulting in trees similar to each other.\n",
    "\n",
    "The main difference between random forest and bagging is that random forest considers only a subset of predictors at a split. This results in trees with different predictors at top split, thereby resulting in decorrelated trees and more reliable average output. That's why we say random forest is robust to correlated predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
