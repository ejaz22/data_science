{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Metrics\n",
    "\n",
    "In some scenarios, we are ok with the overall accuracy whereas in some scenario the cost of misclassifying a single data point is huge. For example In a scenario of bank finding whether a customer is eligible for the loan or not it can be alright if we might misclassify as some eligible customers as not eligible. But in case of a doctor classifying the patients as having cancer or not it would be a blunder if we declare some potential cancer patients as cancer-free.\n",
    "\n",
    "### Confusion Matrix\n",
    "\n",
    "A confusion matrix is a table that is often used to describe the performance of a classification model on a set of test data for which the true values are known. Confusion matrix is nice, but it is not statistically significant as it is a point estimate.\n",
    "\n",
    "- True Positives (TP): Observations where the actual and predicted transactions were fraud\n",
    "\n",
    "- True Negatives (TN): Observations where the actual and predicted transactions weren’t fraud\n",
    "\n",
    "- False Positives (FP) or False Alarm or Type I Error : Observations where the actual transactions weren’t fraud but predicted to be fraud\n",
    "\n",
    "- False Negatives (FN) or Type II Error : Observations where the actual transactions were fraud but weren’t predicted to be fraud\n",
    "- ideal scenario - zero values for FP and FN\n",
    "- Samples in the FP set are actually negatives and samples in FN are actually positives.\n",
    "\n",
    "### Accuracy (Acc)\n",
    "\n",
    "- Acc = (tp + tn) / (tp + tn + fp + fn)\n",
    "- Classification accuracy is the percentage of correct prediction over total instances. Accuracy can be a good metric if the classes are balanced\n",
    "-  it doesn't tell us where the model is making errors. Answering this \"where\" question is an essential part of model-building. \n",
    "\n",
    "### Classification Error / Error Rate / Misclassification Rate (ERR)\n",
    "\n",
    "- ERR = (1-Acc)\n",
    "- measures the ratio of incorrect predictions over the total number of instances evaluated. \n",
    "- applicable for multi-class and multi-label problems;\n",
    "- Another problem with the accuracy is that two classifiers can yield the same accuracy but perform differently\n",
    "- Both accuracy and error rate metrics are sensitive to the imbalanced data. The imbalance dataset makes accuracy, not a reliable performance metric to use. To cope with this problem, we can choose to penalize false positives or false negatives. This will generate two alternative metrics i.e precision and recall.\n",
    "\n",
    "\n",
    "### Precision\n",
    "\n",
    "Precision is the ability of a model to identify only the relevant data points. For example, for a text search on a set of documents, precision is the number of correct results (TP) divided by the number of all returned results (that belongs to the positive class i.e TP + FP)\n",
    "\n",
    "- Precision is the probability that our classifier will properly identify as positive.\n",
    "- The precision is the ability of the classifier not to label as positive when it is negative. With precision, we are evaluating our data by its performance of ‘positive’ predictions.\n",
    "- Recall is TP/acutal_yes whereas Precsion is TP/predicted_yes\n",
    "- Both Precision and Recall do not consider TNs.\n",
    "\n",
    "### Recall\n",
    "\n",
    "Recall is the ability of a model to find all the relevant cases within a training dataset. For example, for a text search on a set of documents, recall is the number of correct results (TP) divided by the number of results that should have been returned (that actually belongs to the positive class i.e. TP+FN). A higher relcall (ie 1.0) says that we will catch every terrorist but our precision will be very low i.e. we will detain many innocents\n",
    "\n",
    "- The recall is the ability of the classifier to find all the positive samples. With recall, we are evaluating our data by its performance of the ground truths for positive outcomes.\n",
    "- e.g. If a sample is positive for the disease, what’s the probability that the system will pick it up\n",
    "\n",
    "### F1 (F-beta measures)\n",
    "\n",
    "Used when the target variable is unbalanced. The F1 score can be interpreted as a weighted average of the precision and recall. We use the harmonic mean instead of a simple average because it punishes extreme values. A classifier with a precision of 1.0 and a recall of 0.0 has a simple average of 0.5 but an F1 score of 0. The F1 score gives equal weight to both measures. \n",
    "\n",
    "When beta takes value of 0.5 then it is called F1 measures. F1 scores can be used to compare two models. F1 is used where true negatives don’t matter much.  The best value for recall, precision and F1 is 1 and the worst value is 0.\n",
    "\n",
    "The difference in F1 score reflects the model performance. When you have a small positive class, then F1 score makes more sense. This is the common problem in fraud detection where positive labels are few.\n",
    "\n",
    "### Sensitivity or TPR or Recall\n",
    "\n",
    "- Sensitivity is True Positive rate and is also called positve recall while specificity (TNR) is called negative recall.\n",
    "\n",
    "### Specificity or TNR is opposite of Recall\n",
    "\n",
    "- = TN/(TN+FP).\n",
    "- Sensitivity and Specificity may give you a biased result, especially for imbalanced classes.\n",
    "- both precision and recall are necessary to determine if the classifier is performing well.\n",
    "\n",
    "\n",
    "### Matthews Correlation Coefficient (MCC)\n",
    "- Similar to Correlation Coefficient and its values lie between -1 to +1. A model with a score of +1 is a perfect model and -1 is a poor model. This property is one of the key usefulness of MCC as it leads to easy interpretability.\n",
    "\n",
    "\n",
    "### ROC Curve (TPR vs 1-specificity aka FPR)\n",
    "\n",
    "ROC is only used in binary classification problem. ROC is a function of threshold which shows the performance of a classification model at all thresholds.It is generated by plotting the True Positive Rate (y-axis) against the False Positive Rate (x-axis)\n",
    "\n",
    " \n",
    "- The ROC Curve/AUC Score is most useful when we are evaluating a model to itself\n",
    "- The value can range from 0 to 1. However auc score of a random classifier for balanced data is 0.5\n",
    "- ROC-AUC score is independent of the threshold set for classification because it only considers the rank of each prediction and not its absolute value. The same is not true for F1 score which needs a threshold value in case of probabilities output\n",
    "\n",
    "### AOC\n",
    "\n",
    "- AUC is one of the popular ranking type metrics AUC is the area under the ROC curve. Perfect classifier: AUC=1, fall on (0,1); 100% sensitivity (no FN) and 100% specificity (no FP)\n",
    "\n",
    "- The probabilistic interpretation of ROC-AUC score is that if we randomly choose a positive case and a negative case, the probability that the positive case outranks the negative case according to the classifier is given by the AUC. Here, rank is determined according to order by predicted values.\n",
    "- ROC-AUC score is independent of the threshold set for classification because it only considers the rank of each prediction and not its absolute value. The same is not true for F1 score which needs a threshold value in case of probabilities output\n",
    "- AUC is the percentage of the ROC plot that is underneath the curve.\n",
    "- The AUC represents a model’s ability to discriminate between positive and negative classes. An area of 1.0 represents a model that made all predictions perfectly. An area of 0.5 represents a model as good as random.\n",
    "- AUC is useful even when there is high class imbalance (unlike classification accuracy)\n",
    "  Fraud case\n",
    "   - Null accuracy almost 99%\n",
    "    - AUC is useful here\n",
    "\n",
    "General AUC predictions:\n",
    "\n",
    "- .90-1 = Excellent\n",
    "- .80-.90 = Good\n",
    "- .70-.80 = Fair\n",
    "- .60-.70 = Poor\n",
    "- .50-.60 = Fail\n",
    "\n",
    "AUC ROC considers the predicted probabilities for determining the model’s performance. But, it only takes into account the order of probabilities and hence it does not take into account the model’s capability to predict higher probability for samples more likely to be positive(Log Loss).\n",
    "\n",
    "Whereas the AUC is computed with regards to binary classification with a varying decision threshold, log loss actually takes “certainty” of classification into account.\n",
    "\n",
    "\n",
    "### Precsion Recall Curve (PRC)\n",
    "\n",
    "When dealing with highly skewed datasets (class imbalance), Precision-Recall (PR) curves give a more informative picture of an algorithm's performance. PRC is also used for binary classification problems. \n",
    "\n",
    "### Log loss/ Logarithmic Loss / Logistic Loss / Cross-Entropy Loss\n",
    "\n",
    "AUC ROC considers the predicted probabilities for determining our model’s performance. However, there is an issue with AUC ROC, it only takes into account the order of probabilities and hence it does not take into account the model’s capability to predict higher probability for samples more likely to be positive. In that case, we could us the log loss which is nothing but negative average of the log of corrected predicted probabilities for each instance. Punishes infinitely the deviation from the true value! It’s better to be somewhat wrong than emphatically wrong!\n",
    "\n",
    " \n",
    "- When working with Log Loss, the classifier must assign probability to each class for all the samples.\n",
    "- Log loss measures the UNCERTAINTY of the probabilities of the model by comparing them to the true labels and penalising the false classifications.\n",
    "- Log loss is only defined for two or more labels.\n",
    "- Log Loss gradually declines as the predicted probability improves, thus Log Loss nearer to 0 indicates higher accuracy, Log Loss away from 0 indicates lower accuracy.\n",
    "- Log Loss exists in the range (0, ∞].\n",
    "\n",
    "### Gini Coefficient or Somers' D statistic\n",
    "\n",
    "- Gini = 2*AUC – 1\n",
    "- Gini above 60% is a good model. For the case in hand we get Gini as 92.7%.\n",
    "\n",
    "### Concordant – Discordant ratio (pairs)\n",
    "\n",
    "Concordant pairs and discordant pairs refer to comparing two pairs of data points to see if they “match.” The meaning is slightly different depending on if you are finding these pairs from various coefficients (like Kendall’s Tau) or if you are performing experimental studies and clinical trials.\n",
    "\n",
    "Concordant Pair: A pair of observations (x1, y1) and (x2, y2) that follows the property\n",
    "- x1 > x2 and y1 > y2 or\n",
    "- x1 < x2 and y1 < y2\n",
    "Discordant Pair: A pair of observations (x1, y1) and (x2, y2) that follows the property\n",
    "- x1 > x2 and y1 < y2 or\n",
    "- x1 < x2 and y1 > y2\n",
    "\n",
    "__Note:__ The pair for which x1 = x2 and y1 = y2 are not classified as concordant or discordant, hence tied and are ignored.\n",
    "\n",
    "To calculate the concordant and discordant pairs, the data are treated as ordinal, so ordinal data should be appropriate for your application. The number of concordant and discordant pairs are used in calculations for Kendall's tau, which measures the association between two ordinal variables. Kendal(tau) is a non-parametric rank based correlation test like spearmans(rho). The other parametric correlation test is Pearson(r).\n",
    "\n",
    "```python\n",
    "from scipy.stats._stats import _kendall_dis\n",
    "```\n",
    "\n",
    "\n",
    "### Choice of Metrics\n",
    "\n",
    "It depends on the business objective and the cost consideration. I consider the largest difference in ROC and PR AUC the fact the ROC is determining how well your model can \"calculate\" the positive class AND the negative class where as the PR AUC is really only looking at your positive class. So in a balanced class situation and where you care about both negative and positive classes, the ROC AUC metric works great. When you have an imbalanced situation, it is preferred to use the PR AUC, but keep in mind it is only determining how well your model can \"calculate\" the positive class!\n",
    "\n",
    "- If we care for absolute probabilistic difference, go with log-loss.\n",
    "- If we care only for the final class prediction and we don’t want to tune threshold, go with AUC score. -F1 score is sensitive to threshold and we would want to tune it first before comparing the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q When will we prefer F1 over ROC-AUC?\n",
    "\n",
    "Prefer PR curve whenever the positive class is rare or when we care more about the false positives than the false negatives.\n",
    "\n",
    "To train binary classifiers, choose the appropriate metric for the task, evaluate the classifiers using cross-validation, select the precision/ recall tradeoff that fits our needs, and compare various models using ROC curves and ROC AUC scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kendal tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4285714285714286"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import required libraries \n",
    "from scipy.stats import kendalltau \n",
    "  \n",
    "# Taking values from the above example in Lists \n",
    "X = [1, 2, 3, 4, 5, 6, 7] \n",
    "Y = [1, 3, 6, 2, 7, 4, 5] \n",
    "\n",
    "corr, _ = kendalltau(X, Y)\n",
    "corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cohen Kappa\n",
    "\n",
    "#### Inter-rater agreement Kappas (coefficient of Agreement)\n",
    "a.k.a. inter-rater reliability or concordance\n",
    "\n",
    "\n",
    "Kappa is similar to Accuracy score, but it takes into account the accuracy that would have happened anyway through random predictions.\n",
    "\n",
    "Kappa = (Observed Accuracy - Expected Accuracy) / (1 - Expected Accuracy)\n",
    "\n",
    "\n",
    "In statistics, inter-rater reliability, inter-rater agreement, or concordance is the degree of agreement among raters. It gives a score of how much homogeneity, or consensus, there is in the ratings given by judges.\n",
    "\n",
    "Kappa Statistic is a metric that compares an observed accuracy with an expected accuracy (Random Chance)\n",
    "- values between 0 to 1\n",
    "\n",
    "Kappa is an important measure on classifier performance, especially on imbalanced data set where accuracy metric does not tell more information\n",
    "\n",
    "Cohen’s kappa: a statistic that measures inter-annotator agreement for qualitative (categorical or nominal) items.\n",
    "\n",
    "Note that Cohen’s Kappa only applied to 2 raters rating the exact same items.Fleiss Extends Cohen’s Kappa to more than 2 raters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [1, 0, 1, 1, 1, 1]\n",
    "y_pred = [1, 0, 1, 1, 1, 1]\n",
    "cohen_kappa_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nltk agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, '0', '1'],\n",
       " [0, '1', '1'],\n",
       " [0, '2', '1'],\n",
       " [1, '0', '1'],\n",
       " [1, '1', '1'],\n",
       " [1, '2', '0'],\n",
       " [2, '0', '0'],\n",
       " [2, '1', '1'],\n",
       " [2, '2', '1']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import agreement\n",
    "rater1 = [1,1,1]\n",
    "rater2 = [1,1,0]\n",
    "rater3 = [0,1,1]\n",
    "\n",
    "taskdata=[[0,str(i),str(rater1[i])] for i in range(0,len(rater1))]+[[1,str(i),str(rater2[i])] for i in range(0,len(rater2))]+[[2,str(i),str(rater3[i])] for i in range(0,len(rater3))]\n",
    "taskdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kappa -0.1666666666666667\n",
      "fleiss -0.2000000000000003\n",
      "alpha -0.1428571428571428\n",
      "scotts -0.28571428571428603\n"
     ]
    }
   ],
   "source": [
    "ratingtask = agreement.AnnotationTask(data=taskdata)\n",
    "print(\"kappa \" +str(ratingtask.kappa()))\n",
    "print(\"fleiss \" + str(ratingtask.multi_kappa()))\n",
    "print(\"alpha \" +str(ratingtask.alpha()))\n",
    "print(\"scotts \" + str(ratingtask.pi()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "kappa_scorer = make_scorer(cohen_kappa_score)\n",
    "grid = GridSearchCV(LinearSVC(), param_grid={'C': [1, 10]}, scoring=kappa_scorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score=nan,\n",
       "             estimator=LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                                 fit_intercept=True, intercept_scaling=1,\n",
       "                                 loss='squared_hinge', max_iter=1000,\n",
       "                                 multi_class='ovr', penalty='l2',\n",
       "                                 random_state=None, tol=0.0001, verbose=0),\n",
       "             iid='deprecated', n_jobs=None, param_grid={'C': [1, 10]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=make_scorer(cohen_kappa_score), verbose=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Classificaion Metrics`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn import metrics, datasets, neighbors\n",
    "import sys\n",
    "import warnings\n",
    "import itertools\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Size :  (500, 20) (500,)\n"
     ]
    }
   ],
   "source": [
    "X,Y  = datasets.make_classification(n_samples=500, n_features=20, n_classes=2, random_state=1)\n",
    "print('Dataset Size : ',X.shape,Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Test Size :  (400, 20) (100, 20) (400,) (100,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.80, test_size=0.20, stratify=Y, random_state=1)\n",
    "print('Train/Test Size : ', X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=0.1, class_weight=None, dual=True, fit_intercept=True,\n",
       "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "          multi_class='ovr', penalty='l2', random_state=1, tol=0.0001,\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "linear_svc = LinearSVC(random_state=1, C=0.1)\n",
    "linear_svc.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificatin Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 1 1 0 0 0 1 0 1 0 0 0 1]\n",
      "[0 1 0 1 1 0 0 0 1 0 0 0 0 0 1]\n",
      "Test Accuracy : 0.930\n",
      "Test Accuracy : 0.930\n",
      "Training Accuracy : 0.953\n"
     ]
    }
   ],
   "source": [
    "Y_preds = linear_svc.predict(X_test)\n",
    "\n",
    "print(Y_preds[:15])\n",
    "print(Y_test[:15])\n",
    "\n",
    "print('Test Accuracy : %.3f'%(Y_preds == Y_test).mean())\n",
    "print('Test Accuracy : %.3f'%linear_svc.score(X_test, Y_test)) ## Score method also evaluates accuracy for classification models.\n",
    "print('Training Accuracy : %.3f'%linear_svc.score(X_train, Y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[47  3]\n",
      " [ 4 46]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "conf_mat = confusion_matrix(Y_test, Y_preds)\n",
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision                                   : 0.939\n",
      "Recall                                      : 0.920\n",
      "F1-Score                                    : 0.929\n",
      "\n",
      "Precision Recall F1-Score Support Per Class : \n",
      " (array([0.92156863, 0.93877551]), array([0.94, 0.92]), array([0.93069307, 0.92929293]), array([50, 50], dtype=int32))\n",
      "\n",
      "Classification Report                       : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.94      0.93        50\n",
      "           1       0.94      0.92      0.93        50\n",
      "\n",
      "    accuracy                           0.93       100\n",
      "   macro avg       0.93      0.93      0.93       100\n",
      "weighted avg       0.93      0.93      0.93       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score, precision_recall_fscore_support\n",
    "\n",
    "print('Precision                                   : %.3f'%precision_score(Y_test, Y_preds))\n",
    "print('Recall                                      : %.3f'%recall_score(Y_test, Y_preds))\n",
    "print('F1-Score                                    : %.3f'%f1_score(Y_test, Y_preds))\n",
    "print('\\nPrecision Recall F1-Score Support Per Class : \\n',precision_recall_fscore_support(Y_test, Y_preds))\n",
    "print('\\nClassification Report                       : ')\n",
    "print(classification_report(Y_test, Y_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curves\n",
    "ROC(Receiver Operating Characteristic) Curve helps better understand the performance of the model when handling an unbalanced dataset. ROC Curve works with the output of prediction function by setting different threshold values to find out different false positives and true positive rates according to the threshold. In the case of SVC, for example, a threshold set for output of decision function is 0 whereas ROC Curve tries various values for thresholds like [2,1,-1,-2] including negative threshold values as well. In the case of LogisticRegression, the default threshold is 0.5 and ROC will try default threshold values. For linear regression, the output is a probability between [0,1] hence threshold is set at 0.5 to differentiate positive/negative classes whereas in case of SVC internal kernel function returns value and threshold is set on that value for making a prediction.\n",
    "\n",
    "__Note:__ It's restricted to binary classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "decision_function = linear_svc.decision_function(X_test)\n",
    "fpr, tpr, thresholds = roc_curve(Y_test, decision_function)\n",
    "acc = linear_svc.score(X_test, Y_test)\n",
    "auc = roc_auc_score(Y_test, linear_svc.decision_function(X_test))\n",
    "\n",
    "with plt.style.context(('ggplot','seaborn')):\n",
    "    plt.figure(figsize=(4,3))\n",
    "    plt.scatter(fpr, tpr, c='blue')\n",
    "    plt.plot(fpr, tpr, label=\"Accuracy:%.2f AUC:%.2f\" % (acc, auc), linewidth=2, c='red')\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate (recall)\")\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a very small decision threshold, there will be few false positives, but also few false negatives, while with a very high threshold, both true positive rate and the false positive rate will be high. So in general, the curve will be from the lower left to the upper right. A diagonal line reflects chance performance, while the goal is to be as much in the top left corner as possible. We want ROC Curve to cover almost 100% area for good performance. 50% area coverage refers to the chance model (random prediction)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision-Recall Curve\n",
    "Precision and Recall helps a lot in case of imbalanced datasets. Plotting different values of precision vs recall by setting different thresholds helps in evaluating the performance of the model better in case of imbalance classes. It does not take into consideration true negatives as it's majority class and True positives represent minority class which has quite a few occurrences.\n",
    "\n",
    "__Note:__ It's restricted to binary classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve, auc,average_precision_score\n",
    "\n",
    "decision_function = linear_svc.decision_function(X_test)\n",
    "precision, recall, thresholds = precision_recall_curve(Y_test, decision_function)\n",
    "acc = linear_svc.score(X_test, Y_test)\n",
    "p_auc = auc(recall, precision)\n",
    "\n",
    "with plt.style.context(('ggplot', 'seaborn')):\n",
    "    plt.figure(figsize=(4,3))\n",
    "    plt.scatter(recall, precision, c='blue')\n",
    "    plt.plot(recall, precision, label=\"Accuray:%.2f, AUC:%.2f\" % (acc, p_auc), linewidth=2, c='red')\n",
    "    plt.hlines(0.5,0.0,1.0, linestyle='dashed', colors=['orange'])\n",
    "    plt.xlabel(\"Recall (Sensitivity)\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title('Precision Recall Curve')\n",
    "    plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision-recall curve totally crashes if our model is not performing well in case of imbalanced dataset. Notice that AUC in case of precison recall curve is 50% and whereas AUC with ROC curve was around 90%. ROC curves sometimes give optimistic results hence its better to consider precision recall curves as well in case of imbalanced datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balanced Accuracy Score\n",
    "It returns an average of recall of each class in classification problem. It's useful to deal with imbalanced datasets.\n",
    "\n",
    "It has parameter adjusted which when set True results are adjusted for a chance so that the random performing model would get a score of 0 and perfect performance will get 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Accuracy          :  0.9299999999999999\n",
      "Balanced Accuracy Adjusted :  0.8599999999999999\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "print('Balanced Accuracy          : ',balanced_accuracy_score(Y_test, Y_preds))\n",
    "print('Balanced Accuracy Adjusted : ',balanced_accuracy_score(Y_test, Y_preds, adjusted=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero One Classification Loss\n",
    "It returns a number of misclassifications or a fraction of misclassifications. It accepts normalize parameter whose value if set True then returns a fraction of misclassifications else if set to False then it returns misclassifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Misclassificied Examples   :  7\n",
      "Fraction of Misclassificied Examples :  0.06999999999999995\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import zero_one_loss\n",
    "\n",
    "print('Number of Misclassificied Examples   : ',zero_one_loss(Y_test, Y_preds, normalize=False))\n",
    "print('Fraction of Misclassificied Examples : ',zero_one_loss(Y_test, Y_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brier Loss\n",
    "It computes squared differences between the actual labels of class and predicted probability by model. It should be as low as possible for good performance. It’s for binary classification problems only. It by defaults takes 1 as positive class hence if one needs to consider 0 as a positive class then one can use the pos_label parameter as below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.metrics import brier_score_loss\n",
    "\n",
    "print('Brier Loss                       : ',brier_score_loss(Y_test, grid.predict_proba(X_test)[:, 1]))\n",
    "print('Brier Loss (0 as Positive Class) : ', brier_score_loss(Y_test, grid.predict_proba(X_test)[:, 0], pos_label=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F-Beta Score\n",
    "F-Beta score refers to weighted average of precision and recall based on the value of the beta parameter provided. If beta < 1 then it lends more weight to precision, while beta > 1 lends more weight to recall. It has the best value of 1.0 and the worst 0.0.\n",
    "\n",
    "It has a parameter called average which is required for multiclass problems. It accepts values [None, 'binary'(default), 'micro', 'macro', 'samples', 'weighted']. If None is specified then the score for each class is returned else average as per parameter is returned in a multiclass problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fbeta Favouring Precision :  0.9349593495934959\n",
      "Fbeta Favouring Recall    :  0.9236947791164658\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "print('Fbeta Favouring Precision : ', fbeta_score(Y_test, Y_preds, beta=0.5))\n",
    "print('Fbeta Favouring Recall    : ' ,fbeta_score(Y_test, Y_preds, beta=2.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hamming Loss\n",
    "It returns fraction of labels misclassified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming Loss :  0.07\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import hamming_loss\n",
    "\n",
    "print('Hamming Loss : ', hamming_loss(Y_test, Y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
