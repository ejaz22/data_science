{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "uses of nlp\n",
    "- automatic text summarization, \n",
    "- translation, \n",
    "- named entity recognition, \n",
    "- relationship extraction, \n",
    "- sentiment analysis,\n",
    "- speech recognition,\n",
    "- and topic segmentation / extraction\n",
    "- relationship extration\n",
    "- automated question answering\n",
    "\n",
    "modules\n",
    "- coreNLP\n",
    "- nltk\n",
    "- spacy\n",
    "- prodigy\n",
    "- gensim (topic modelling and document similary comparision)\n",
    "- textblob\n",
    "- stanford NLP\n",
    "- Mallet\n",
    "-  Vowpal Wabbit library\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "19 Great Articles About Natural Language Processing (NLP):\n",
    "\n",
    "Structuring Unstructured Big Data via Indexation\n",
    "\n",
    "Your Guide to Natural Language Processing (NLP)\n",
    "\n",
    "Comparison of Top 6 Python NLP Libraries\n",
    "\n",
    "Text Classification & Sentiment Analysis tutorial\n",
    "\n",
    "Deep Learning Research Review: Natural Language Processing\n",
    "\n",
    "10 Common NLP Terms Explained for the Text Analysis Novice\n",
    "\n",
    "Temporal Convolutional Nets Take Over from RNNs for NLP Predictions\n",
    "\n",
    "How I used NLP (Spacy) to screen Data Science Resumes\n",
    "\n",
    "Data Science Reveals Trump Tweets are Written by Two People\n",
    "\n",
    "Simple introduction to Natural Language Processing\n",
    "\n",
    "An NLP Approach to Analyzing Twitter, Trump, and Profanity\n",
    "\n",
    "A Natural Language Processing (NLP) Approach to Data Exploration\n",
    "\n",
    "Python NLTK Tools List for Natural Language Processing\n",
    "\n",
    "NLP app to find great available domain names\n",
    "\n",
    "Scaling an NLP problem without using a ton of hardware\n",
    "\n",
    "Analyzing the structure and effectiveness of news headlines\n",
    "\n",
    "Seven tricky sentences for NLP and text mining algorithms\n",
    "\n",
    "Overview of Artificial Intelligence and Role of NLP\n",
    "\n",
    "Text Classification & Sentiment Analysis tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glove\n",
    "\n",
    "https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "GloVe stands for \"Global Vectors for Word Representation\". It's a somewhat popular embedding technique based on factorizing a matrix of word co-occurence statistics. GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "\n",
    "__1) Naive Bayes__ -- best applied to a data set containing multiple features (independent variable) and an output variable which takes two discrete value (Yes/No). Thus, categorical data.\n",
    "\n",
    "__2) SVM__ -- best applied to a data set containing infinite number of features and you need to reduce these features down to a number so that it can be computed. Since it's a classification algorithm so it best works upon categorical data.\n",
    "\n",
    "__3) Regression__ -- Linear Regression is applied to a continuous numerical data set in which the dependent and independent variable exhibits linear relationship. For example, size of the house vs house price. Logistic Regression is a classification algorithm so it is best applied to categorical data.\n",
    "\n",
    "__3) K-Means__ -- K-Means can applied to many types of data sets. What it does is segmenting data points into clusters. Data points with similar features are clustered together.\n",
    "\n",
    "__4) Neural Networks__ -- Neural Networks can be shallow neural networks and deep neural networks and both of these could be applied to supervised or unsupervised problem as it has separate algorithms for both the cases. It is the most powerful and popular class of ML algorithms. It can be used in every problem statement. Main intuition behind it learning from its own error.\n",
    "\n",
    "__Covariance__\n",
    "\n",
    "- Covariance is measure of the tendency of two variables to vary together\n",
    "- So covariance is maximized if two vectors are identical\n",
    "- Covariance is zero if they are orthogonal.\n",
    "- Covariance is negative if they point in opposite direction"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Natural Language Processing with Python by Steven Bird\n",
    "\n",
    "Further, the best way to learn is almost certainly to actually implement NLP algorithms from scratch. You could pick some standard tasks (language modeling, text classification, POS-tagging, NER, parsing) and implement various algorithms from the ground up (ngram models, HMMs, Naive Bayes, MaxEnt, CKY) to really understand what makes them work. It also shouldn't be too hard to find some free dataset to test your implementations on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Topic models\n",
    "- NMF : Non-negative Matrix Factorization\n",
    "- LDA : Latend Dirichlet Allocation\n",
    "- Cosine Similarity\n",
    "- word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A matrix with mostly empty cells is called sparse, and the opposite to that (a mostly filled matrix) is called dense. matrix factorization better at dealing with sparsity. If the matrix is mostly empty, reducing dimensions can improve the performance of the algorithm in terms of both space and time. One of the popular algorithms to factorize a matrix is the singular value decomposition (SVD) algorithm. Other algorithms include PCA and its variations, NMF.\n",
    "\n",
    " \n",
    "Correlation is a statistical measure that indicates the extent to which two or more variables fluctuate together. Movies that have a high correlation coefficient are the movies that are most similar to each other. In our case we shall use the Pearson correlation coefficient. This number will lie between -1 and 1. 1 indicates a positive linear correlation while -1 indicates a negative correlation. 0 indicates no linear correlation. Therefore movies with a zero correlation are not similar at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Vectorization\n",
    "\n",
    "Simply calculating the frequency of terms as in document-term matrix suffers from a critical problem, all terms are considered equally important when it comes to assessing relevancy on a query.\n",
    "\n",
    "__Named Entity Recognition (NER)__\n",
    "\n",
    "The process of locating and classifying elements in text into predefined categories such as the names of people, organizations, places, monetary values, percentages, etc.\n",
    "\n",
    "__Tokenization__\n",
    "\n",
    "splitting a string into a list of \"tokens\" / words.\n",
    "\n",
    "__Normalization__\n",
    "\n",
    "Normalization generally refers to a series of related tasks meant to put all text on a level playing field: converting all text to the same case (upper or lower), removing punctuation, expanding contractions, converting numbers to their word equivalents, and so on. Normalization puts all words on equal footing, and allows processing to proceed uniformly.\n",
    "\n",
    "__Stemming__\n",
    "\n",
    "Stemming is the process of eliminating affixes (suffixed, prefixes, infixes, circumfixes) from a word in order to obtain a word stem.\n",
    "\n",
    "running ===> run\n",
    "\n",
    "__Lemmatization__\n",
    "\n",
    "Lemmatization is related to stemming, differing in that lemmatization is able to capture canonical forms based on a word's lemma. For example, stemming the word \"better\" would fail to return its citation form (another word for lemma); however, lemmatization would result in the following:\n",
    "\n",
    "better ===> good\n",
    "\n",
    "\n",
    "__Stop Words__\n",
    "\n",
    "Stop words are those words which are filtered out before further processing of text, since these words contribute little to overall meaning, given that they are generally the most common words in a language. For instance, \"the,\" \"and,\" and \"a,\" \n",
    "\n",
    "__Dcoument__\n",
    "\n",
    "- a document is a set of terms.\n",
    "\n",
    "__Corpus__\n",
    "\n",
    "- a corpus is a set of document.\n",
    "\n",
    "__Bag of Words (BoW).__\n",
    "\n",
    "Bag of Words (BoW) is an algorithm that counts how many times a word appears in a document.\n",
    "\n",
    "- bag referring to the set theory concept of multisets, which differ from simple sets\n",
    "- The bag of words model omits grammar and word order, but is interested in the number of occurrences of words within the text.\n",
    "- BoW is simply a word occurence as how many times a word appears in a document\n",
    "- BoW has limitations such as large feature dimension, sparse representation etc. \n",
    "- if dataset is small and context is domain specific, BoW may work better than Word Embedding\n",
    "- In simple bag-of-words (e.g uni-gram, bi-gram, tri-gram) representations,the frequency (or asimilar weight such as term frequency inverse document frequency) of each word or n-gram is considered as a separate feature. \n",
    "- Bag of words models encode every word in the vocabulary as one-hot-encoded vector \n",
    "- Bag of word models don’t respect semantics of the word. For example: words ‘car’ and ‘automobile’ are often used in the same context.\n",
    "- While modeling phrases using bag-of-words the order of words in the phrase is not respected. Ex: “This is good” and “Is this good” have exactly the same vector representation. BoW lose all information about word order: “John likes Mary” and “Mary likes John” correspond to identical vectors. There is a solution: bag of n-grams models consider word phrases of length n to represent documents as fixed-length vectors to capture local word order but suffer from data sparsity and high dimensionality. The Word2Vec model addresses this second problem.\n",
    "\n",
    "- CountVectorizer()\n",
    "\n",
    "\n",
    "__Bag of n_grams__\n",
    "\n",
    "we use a bag of n-grams as additional features to capture some partial information about the local word order\n",
    "\n",
    "\n",
    "__Term Frequency - measures frequency__\n",
    "\n",
    "It is the ratio of number of times a word occurred in a document to the total number of words in the document. It is also called Normalize count occurence\n",
    "\n",
    "\n",
    "__Inverse Document Frequency(IDF)__\n",
    "\n",
    "-  IDF (inverse document frequency) assumes that the importance of a term is inversely proportional to the frequency of occurrence of this term in all the documents \n",
    "\n",
    "- it is the logarithm of (total number of documents divided by number of documents containing the word).\n",
    "\n",
    "__TF-IDF - measures relevance__\n",
    "\n",
    "TF-IDF approach believe that high frequency may not able to provide much information gain. In another words, rare words contribute more weights to the model.\n",
    "\n",
    "- tf-idf helps you rank the importance of a term\n",
    "- With TF-IDF, words are given weight – TF-IDF measures relevance, not frequency\n",
    "- Term frequency (tf) is basically the output of the BoW model\n",
    "- TF-IDF can achieve better results than simple term frequencies when combined with some supervised methods.\n",
    "- TfidfVectorizer Converts a collection of raw documents to a matrix of TF-IDF features. \n",
    "- Tfidfvectorizer are feature generation algorithms and hence running on test will not cause overfitting or leakage\n",
    "- Tf-idf is a scoring scheme for words - that is a measure of how important a word is to a document.\n",
    "- TF-IDF is a way to judge the topic of an article. This is done by the kind of words it contains. Here words are given weight so it measures relevance, not frequency.\n",
    "\n",
    "\n",
    "__Word Embedding Word2Vec__\n",
    "\n",
    "Word embedding is a vector representation of a word. Word2Vec is a more sophisticated word embedding technique. This technique is based on the idea that words that occur in the same contexts tend to have similar meanings.\n",
    "\n",
    "- two types : CBOW and Skip Grams\n",
    "- also known as Skip-gram with Negative Sampling (SGNS)\n",
    "- Word2vec produces one vector per word, whereas tf-idf produces a score. \n",
    "- Word2vec produces one vector per word, whereas BoW produces one number (a wordcount)\n",
    "- word2vec learns relationships between words automatically.\n",
    "- Gensim is heavily applied for training word2vec and doc2vec\n",
    "- can solve analogies cleary\n",
    "- gensim.models.Word2Vec()\n",
    "\n",
    "__Doc2Vec__\n",
    "\n",
    "- Doc2Vec is a Model that represents each Document as a Vector (paragraph embedding)\n",
    "\n",
    "\n",
    "__DeepIR__\n",
    "\n",
    "__Global Vectors (GloVe)__\n",
    "- GloVe is modeled to do dimensionality reduction.\n",
    "- Glove and Word2vec are both unsupervised models for generating word vectors\n",
    "\n",
    "__Cosine Similarity__\n",
    "\n",
    "- With cosine similarity we can measure the similarity between two document vectors\n",
    "- cosine similarity == 1 ==> same document. If it is 0, the documents share nothing. \n",
    "- compare different documents with cosine similarity or the Euclidean dot product formula.\n",
    "\n",
    "__Latent Dirichlet Allocation (LDA)__\n",
    "\n",
    "- Latent Dirichlet Allocation, LDA is yet another transformation from bag-of-words counts into a topic space of lower dimensionality. LDA is a probabilistic extension of LSA (also called multinomial PCA), so LDA’s topics can be interpreted as probability distributions over words. These distributions are, just like with LSA, inferred automatically from a training corpus. Documents are in turn interpreted as a (soft) mixture of these topics (again, just like with LSA)\n",
    "\n",
    "-  LDA model didn’t do a very good job of classifying our documents into topics. This is mostly because the corpus used to train the LDA model is so small. Using a larger corpus should give you much better results\n",
    "\n",
    "\n",
    "\n",
    "__Latent Semantic Analysis (LSA)__\n",
    "\n",
    "- Both LSA and LDA have same input which is Bag of words in matrix format. LSA focus on reducing matrix dimension while LDA solves topic modeling problems.\n",
    "\n",
    "__Positive Pointwise Mutual Information (PPMI)__\n",
    "\n",
    "- PMI is a typical measure for the strength of association between two words.\n",
    "\n",
    "__Singular Value Decomposition__\n",
    "\n",
    "- SVD is among the more popular methods for dimensionality reduction and came about in NLP originally via latent semantic analysis (LSA)\n",
    "- The challenge of SVD is that we are hard to determine the optimal number of dimension\n",
    "\n",
    "__Random Projections(RP)__\n",
    "\n",
    "RP aim to reduce vector space dimensionality. This is a very efficient (both memory- and CPU-friendly) approach to approximating TfIdf distances between documents, by throwing in a little randomness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q. What is NLP preprocessing steps?__\n",
    "\n",
    "We first clean the text by removing HTML tags. Next, we tokenize the message’s sentences and remove stopwords. Then, we conduct lemmatization to convert words in different inflected forms into the same base form. Finally, we convert the documents into a collection of words (a so-called bag of words) and build a dictionary of those words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there are two kinds of works involved in text representation: indexing and term weighting. Indexing is the job to assign the indexing terms for the documents\n",
    "\n",
    "Term weighting is the job to assign the weights of terms which measure the importance of terms in documents\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q 19. What is: collaborative filtering, n-grams, cosine distance?__\n",
    "\n",
    "Collaborative filtering:\n",
    "- Technique used by some recommender systems\n",
    "- Filtering for information or patterns using techniques involving collaboration of multiple agents: viewpoints, data sources.\n",
    "1. A user expresses his/her preferences by rating items (movies, CDs.)\n",
    "2. The system matches this user’s ratings against other users’ and finds people with most similar tastes\n",
    "3. With similar users, the system recommends items that the similar users have rated highly but not yet being rated by this user\n",
    "\n",
    "n-grams:\n",
    "- Contiguous sequence of n items from a given sequence of text or speech\n",
    "- “Andrew is a talented data scientist”\n",
    "- Bi-gram: “Andrew is”, “is a”, “a talented”.\n",
    "- Tri-grams: “Andrew is a”, “is a talented”, “a talented data”.\n",
    "- An n-gram model models sequences using statistical properties of n-grams; see: Shannon Game\n",
    "- More concisely, n-gram model: P(Xi|Xi−(n−1)...Xi−1): Markov model\n",
    "- N-gram model: each word depends only on the n−1 last words\n",
    "\n",
    "Issues:\n",
    "- when facing infrequent n-grams\n",
    "- solution: smooth the probability distributions by assigning non-zero probabilities to unseen words or n-grams\n",
    "- Methods: Good-Turing, Backoff, Kneser-Kney smoothing\n",
    "\n",
    "Cosine distance:\n",
    "- How similar are two documents?\n",
    "- Perfect similarity/agreement: 1\n",
    "- No agreement : 0 (orthogonality)\n",
    "- Measures the orientation, not magnitude\n",
    "\n",
    "Given two vectors A and B representing word frequencies:\n",
    "cosine-similarity(A,B)=⟨A,B⟩||A||⋅||B||\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q. How would you come up with a solution to identify plagiarism?__\n",
    "\n",
    "- Vector space model approach\n",
    "- Represent documents (the suspect and original ones) as vectors of terms\n",
    "- Terms: n-grams; n=1 to as much we can (detect passage plagiarism)\n",
    "- Measure the similarity between both documents\n",
    "- Similarity measure: cosine distance, Jaro-Winkler, Jaccard\n",
    "- Declare plagiarism at a certain threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.004504527406047898"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Statistical distance\n",
    "\n",
    "from scipy import spatial\n",
    "\n",
    "a = [1, 2]\n",
    "b = [2, 4]\n",
    "c = [2.5, 4]\n",
    "d = [4.5, 5]\n",
    "\n",
    "spatial.distance.euclidean(c, a)\n",
    "#2.5\n",
    "spatial.distance.euclidean(c, b)\n",
    "#0.5\n",
    "spatial.distance.euclidean(c, d)\n",
    "#2.23606797749979\n",
    "\n",
    "spatial.distance.cosine(c,a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__RF parameters tuning__\n",
    "\n",
    "n_estimators - the number of decision trees in a forest\n",
    "\n",
    "max_depth - the depth of a tree\n",
    "\n",
    "min_samples_leaf - the minimum sample in the leaf. The node with less than this limit is not allowed to split\n",
    "\n",
    "max_features - a fraction (from 0 to 1) of features to be considered in a tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
