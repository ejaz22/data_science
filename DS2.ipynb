{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The very objective ML task is to minimize the loss function on training dataset. An optimization algorithms can then minimize this loss.\n",
    "- here exist a point which gives the minimum error.\n",
    "- Objective function consist of training loss and regularization i.e. Obj(loss,regularization)\n",
    "- Training Loss measures how well model fit on training data\n",
    "- Regularization, measures complexity of model or how how complicated the model is.\n",
    "\n",
    "- Optimizing training loss results in good fit, whereas optimizing regularization encourages simple model.\n",
    "\n",
    "- Simple models means smaller variance.\n",
    "\n",
    "- Overfitting is when a model is trained too well on the training dat\n",
    "\n",
    "- For which problems will you use L1 penalty, and for which - L2?\n",
    "\n",
    "__Convex Optimization__ \n",
    "- Property : strictly convex functions have a unique global minimum,\n",
    "- linear regression/ Ridge regression, with Tikhonov regularisation, etc\n",
    "- sparse linear regression with L1 regularisation, such as lasso\n",
    "- support vector machines\n",
    "- parameter estimation in linear-Gaussian time series (Kalman filter and friends)\n",
    "- The typical cost functions you encounter (cross entropy, absolute loss, least squares) are designed to be convex.\n",
    "\n",
    "__Concav function__\n",
    "- A concave function is the negative of a convex function. Take âˆ’x2, for example\n",
    "\n",
    "\n",
    "__non-convex optimisation__\n",
    "\n",
    "- a Non-convex functions may have several local minima, Therefore, in a non-convex problem, there is usually no way to test if the solution the best solution.\n",
    "- eg. neural networks, maximum likelihood mixtures of Gaussians\n",
    "\n",
    "\n",
    "\n",
    "- Linear algorithms (linear regression, logistic regression etc) will give you convex solutions, that is they will converge. When using neural nets with hidden layers however, you are no longer guaranteed a convex solution.\n",
    "\n",
    "\n",
    "\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q. When should one use median, as opposed to the mean or average?__\n",
    "\n",
    "It really depends on the distribution of the data and the question you are trying to address.\n",
    "\n",
    "for a symmetrical data mean and median is same\n",
    "\n",
    "Fore skewed daa, we take median as it is less susceptible to outliers and extreme values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q. What is the difference between a parametric learning algorithm and a nonparametric learning algorithm?__\n",
    "\n",
    "non-parametric does not mean that they have NO parameters\n",
    "\n",
    "in a parametric model, we have a finite number of parameters, and in nonparametric models, the number of parameters is (potentially) infinite. Or in other words, in nonparametric models, the complexity of the model grows with the number of training data; in parametric models, we have a fixed number of parameters (or a fixed structure if you will).\n",
    "\n",
    "Linear models such as linear regression, logistic regression, and linear Support Vector Machines are typical examples of a parametric \"learners;\" here, we have a fixed size of parameters (the weight coefficient.) In contrast, K-nearest neighbor, decision trees, or RBF kernel SVMs are considered as non-parametric learning algorithms since the number of parameters grows with the size of the training set. -- K-nearest neighbor and decision trees, that makes sense, but why is an RBF kernel SVM non-parametric whereas a linear SVM is parametric? In the RBF kernel SVM, we construct the kernel matrix by computing the pair-wise distances between the training points, which makes it non-parametric.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q What is Euclidean distance in terms of machine learning?__\n",
    "\n",
    "It is just a distance measure between a pair of samples p and q in an n-dimensional feature space:\n",
    "\n",
    "\n",
    "For example, picture it as a \"straight, connecting\" line in a 2D feature space:\n",
    "\n",
    "\n",
    "\n",
    "The Euclidean is often the \"default\" distance used in e.g., K-nearest neighbors (classification) or K-means (clustering) to find the \"k closest points\" of a particular sample point. \n",
    "\n",
    "Another prominent example is hierarchical clustering, agglomerative clustering (complete and single linkage) where you want to find the distance between clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q How can we convert continuous variable to categorical variable__\n",
    "\n",
    "Using discretization of data such as entropy (Expected Information) based binning methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q What are the different dimensionality reduction methods in machine learning?__\n",
    "\n",
    "Since there are so many different approaches, let's break it down to \"feature selection\" and \"feature extraction.\"\n",
    "\n",
    "Some examples of feature selection:\n",
    "\n",
    "__L1 regularization (e.g., Logistic regression) and sparsity__\n",
    "\n",
    "- variance thresholds\n",
    "- recursive feature elimination based on the weights of linear models\n",
    "- random forests / extra trees and feature importance (calculated as average information gain)\n",
    "- sequential forward/backward selection\n",
    "- genetic algorithms\n",
    "- exhaustive search\n",
    "\n",
    "__Some examples of feature extraction:__\n",
    "\n",
    "- Principal Component Analysis (PCA), unsupervised, returns axes of maximal variance given the constraint that those axes are orthogonal to each other\n",
    "- Linear Discriminant Analysis (LDA; not to be confused with Latent Dirichlett Allocation), supervised, returns axes that maximizes class separability (same constraint that axes are also orthogonal); and another article: Linear Discriminant Analysis bit by bit\n",
    "- kernel PCA: uses kernel trick to transform non-linear data to a feature space were samples may be linearly separable (in contrast, LDA and PCA are linear transformation techniques\n",
    "- supervised PCA\n",
    "- and many more non-linear transformation techniques, which you can find nicely summarized here: Nonlinear dimensionality reduction\n",
    "\n",
    "__So, which technique should we use?__\n",
    "\n",
    "This also follows the \"No Lunch Theorem\" principle in some sense: there is no method that is always superior; it depends on your dataset. Intuitively, LDA would make more sense than PCA if you have a linear classification task, but empirical studies showed that it is not always the case. Although kernel PCA can separate concentric circles, it fails to unfold the Swiss Rroll, for example; here, locally linear embedding (LLE) would be more appropriate.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
