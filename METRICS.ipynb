
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Metrics\n",
    "\n",
    "In some scenarios, we are ok with the overall accuracy whereas in some scenario the cost of misclassifying a single data point is huge. For example In a scenario of bank finding whether a customer is eligible for the loan or not it can be alright if we might misclassify as some eligible customers as not eligible. But in case of a doctor classifying the patients as having cancer or not it would be a blunder if we declare some potential cancer patients as cancer-free.\n",
    "\n",
    "__Confusion Matrix__\n",
    "\n",
    "A confusion matrix is a table that is often used to describe the performance of a classification model on a set of test data for which the true values are known. Confusion matrix is nice, but it is not statistically significant as it is a point estimate.\n",
    "\n",
    "\n",
    "__Confusion matrix Terms:__\n",
    "- True Positives (TP): Observations where the actual and predicted transactions were fraud\n",
    "\n",
    "- True Negatives (TN): Observations where the actual and predicted transactions weren’t fraud\n",
    "\n",
    "- False Positives (FP) or False Alarm or Type I Error : Observations where the actual transactions weren’t fraud but predicted to be fraud\n",
    "\n",
    "- False Negatives (FN) or Type II Error : Observations where the actual transactions were fraud but weren’t predicted to be fraud\n",
    "- ideal scenario - zero values for FP and FN\n",
    "- Samples in the FP set are actually negatives and samples in FN are actually positives.\n",
    "\n",
    "__Accuracy (Acc)__\n",
    "\n",
    "- Acc = (tp + tn) / (tp + tn + fp + fn)\n",
    "- Classification accuracy is the percentage of correct prediction over total instances. Accuracy can be a good metric if the classes are balanced\n",
    "-  it doesn't tell us where the model is making errors. Answering this \"where\" question is an essential part of model-building. \n",
    "\n",
    "__Classification Error / Error Rate / Misclassification Rate (ERR)__\n",
    "\n",
    "- ERR = (1-Acc)\n",
    "- measures the ratio of incorrect predictions over the total number of instances evaluated. \n",
    "- applicable for multi-class and multi-label problems;\n",
    "- Another problem with the accuracy is that two classifiers can yield the same accuracy but perform differently\n",
    "- Both accuracy and error rate metrics are sensitive to the imbalanced data. The imbalance dataset makes accuracy, not a reliable performance metric to use. To cope with this problem, we can choose to penalize false positives or false negatives. This will generate two alternative metrics i.e precision and recall.\n",
    "\n",
    "\n",
    "__Precision__\n",
    "\n",
    "Precision is the ability of a model to identify only the relevant data points. For example, for a text search on a set of documents, precision is the number of correct results (TP) divided by the number of all returned results (that belongs to the positive class i.e TP + FP)\n",
    "\n",
    "- Precision is the probability that our classifier will properly identify as positive.\n",
    "- The precision is the ability of the classifier not to label as positive when it is negative. With precision, we are evaluating our data by its performance of ‘positive’ predictions.\n",
    "- Recall is TP/acutal_yes whereas Precsion is TP/predicted_yes\n",
    "- Both Precision and Recall do not consider TNs.\n",
    "\n",
    "__Recall__\n",
    "\n",
    "Recall is the ability of a model to find all the relevant cases within a training dataset. For example, for a text search on a set of documents, recall is the number of correct results (TP) divided by the number of results that should have been returned (that actually belongs to the positive class i.e. TP+FN). A higher relcall (ie 1.0) says that we will catch every terrorist but our precision will be very low i.e. we will detain many innocents\n",
    "\n",
    "- The recall is the ability of the classifier to find all the positive samples. With recall, we are evaluating our data by its performance of the ground truths for positive outcomes.\n",
    "- e.g. If a sample is positive for the disease, what’s the probability that the system will pick it up\n",
    "\n",
    "__F1 (F-beta measures)__\n",
    "\n",
    "Used when the target variable is unbalanced. The F1 score can be interpreted as a weighted average of the precision and recall. We use the harmonic mean instead of a simple average because it punishes extreme values. A classifier with a precision of 1.0 and a recall of 0.0 has a simple average of 0.5 but an F1 score of 0. The F1 score gives equal weight to both measures. \n",
    "\n",
    "When beta takes value of 0.5 then it is called F1 measures. F1 scores can be used to compare two models. F1 is used where true negatives don’t matter much.  The best value for recall, precision and F1 is 1 and the worst value is 0.\n",
    "\n",
    "The difference in F1 score reflects the model performance. When you have a small positive class, then F1 score makes more sense. This is the common problem in fraud detection where positive labels are few.\n",
    "\n",
    "__Sensitivity or TPR or Recall__\n",
    "\n",
    "- Sensitivity is True Positive rate and is also called positve recall while specificity (TNR) is called negative recall.\n",
    "\n",
    "__Specificity or TNR is opposite of Recall__\n",
    "\n",
    "- = TN/(TN+FP).\n",
    "- Sensitivity and Specificity may give you a biased result, especially for imbalanced classes.\n",
    "- both precision and recall are necessary to determine if the classifier is performing well.\n",
    "\n",
    "\n",
    "__Matthews Correlation Coefficient (MCC)__\n",
    "- Similar to Correlation Coefficient and its values lie between -1 to +1. A model with a score of +1 is a perfect model and -1 is a poor model. This property is one of the key usefulness of MCC as it leads to easy interpretability.\n",
    "\n",
    "\n",
    "__ROC Curve (TPR vs 1-specificity aka FPR)__\n",
    "\n",
    "ROC is only used in binary classification problem. ROC is a function of threshold which shows the performance of a classification model at all thresholds.It is generated by plotting the True Positive Rate (y-axis) against the False Positive Rate (x-axis)\n",
    "\n",
    " \n",
    "- The ROC Curve/AUC Score is most useful when we are evaluating a model to itself\n",
    "- The value can range from 0 to 1. However auc score of a random classifier for balanced data is 0.5\n",
    "- ROC-AUC score is independent of the threshold set for classification because it only considers the rank of each prediction and not its absolute value. The same is not true for F1 score which needs a threshold value in case of probabilities output\n",
    "\n",
    "__AOC__\n",
    "\n",
    "- AUC is one of the popular ranking type metrics AUC is the area under the ROC curve. Perfect classifier: AUC=1, fall on (0,1); 100% sensitivity (no FN) and 100% specificity (no FP)\n",
    "\n",
    "- The probabilistic interpretation of ROC-AUC score is that if we randomly choose a positive case and a negative case, the probability that the positive case outranks the negative case according to the classifier is given by the AUC. Here, rank is determined according to order by predicted values.\n",
    "- ROC-AUC score is independent of the threshold set for classification because it only considers the rank of each prediction and not its absolute value. The same is not true for F1 score which needs a threshold value in case of probabilities output\n",
    "- AUC is the percentage of the ROC plot that is underneath the curve.\n",
    "- The AUC represents a model’s ability to discriminate between positive and negative classes. An area of 1.0 represents a model that made all predictions perfectly. An area of 0.5 represents a model as good as random.\n",
    "- AUC is useful even when there is high class imbalance (unlike classification accuracy)\n",
    "  Fraud case\n",
    "   - Null accuracy almost 99%\n",
    "    - AUC is useful here\n",
    "\n",
    "General AUC predictions:\n",
    "\n",
    "- .90-1 = Excellent\n",
    "- .80-.90 = Good\n",
    "- .70-.80 = Fair\n",
    "- .60-.70 = Poor\n",
    "- .50-.60 = Fail\n",
    "\n",
    "AUC ROC considers the predicted probabilities for determining the model’s performance. But, it only takes into account the order of probabilities and hence it does not take into account the model’s capability to predict higher probability for samples more likely to be positive(Log Loss).\n",
    "\n",
    "Whereas the AUC is computed with regards to binary classification with a varying decision threshold, log loss actually takes “certainty” of classification into account.\n",
    "\n",
    "\n",
    "__Precsion Recall Curve (PRC)__\n",
    "\n",
    "When dealing with highly skewed datasets (class imbalance), Precision-Recall (PR) curves give a more informative picture of an algorithm's performance. PRC is also used for binary classification problems. \n",
    "\n",
    "__Log loss/ Logarithmic Loss / Logistic Loss / Cross-Entropy Loss__\n",
    "\n",
    "AUC ROC considers the predicted probabilities for determining our model’s performance. However, there is an issue with AUC ROC, it only takes into account the order of probabilities and hence it does not take into account the model’s capability to predict higher probability for samples more likely to be positive. In that case, we could us the log loss which is nothing but negative average of the log of corrected predicted probabilities for each instance. Punishes infinitely the deviation from the true value! It’s better to be somewhat wrong than emphatically wrong!\n",
    "\n",
    " \n",
    "- When working with Log Loss, the classifier must assign probability to each class for all the samples.\n",
    "- Log loss measures the UNCERTAINTY of the probabilities of the model by comparing them to the true labels and penalising the false classifications.\n",
    "- Log loss is only defined for two or more labels.\n",
    "- Log Loss gradually declines as the predicted probability improves, thus Log Loss nearer to 0 indicates higher accuracy, Log Loss away from 0 indicates lower accuracy.\n",
    "- Log Loss exists in the range (0, ∞].\n",
    "\n",
    "__Gini Coefficient__\n",
    "\n",
    "- Gini = 2*AUC – 1\n",
    "- Gini above 60% is a good model. For the case in hand we get Gini as 92.7%.\n",
    "\n",
    "__Concordant – Discordant ratio__\n",
    "\n",
    "\n",
    "__kohens kappa__\n",
    "\n",
    "Kappa is similar to Accuracy score, but it takes into account the accuracy that would have happened anyway through random predictions.\n",
    "\n",
    "Kappa = (Observed Accuracy - Expected Accuracy) / (1 - Expected Accuracy)\n",
    "\n",
    "__Choice of Metrics__\n",
    "\n",
    "It depends on the business objective and the cost consideration. I consider the largest difference in ROC and PR AUC the fact the ROC is determining how well your model can \"calculate\" the positive class AND the negative class where as the PR AUC is really only looking at your positive class. So in a balanced class situation and where you care about both negative and positive classes, the ROC AUC metric works great. When you have an imbalanced situation, it is preferred to use the PR AUC, but keep in mind it is only determining how well your model can \"calculate\" the positive class!\n",
    "\n",
    "- If we care for absolute probabilistic difference, go with log-loss.\n",
    "- If we care only for the final class prediction and we don’t want to tune threshold, go with AUC score. -F1 score is sensitive to threshold and we would want to tune it first before comparing the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q When will we prefer F1 over ROC-AUC?__\n",
    "\n",
    "Prefer PR curve whenever the positive class is rare or when we care more about the false positives than the false negatives.\n",
    "\n",
    "To train binary classifiers, choose the appropriate metric for the task, evaluate the classifiers using cross-validation, select the precision/ recall tradeoff that fits our needs, and compare various models using ROC curves and ROC AUC scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Metrics\n",
    "\n",
    "\n",
    "- RMSE (root mean squared error), \n",
    "- MAE (mean absolute error), \n",
    "- WMAE(weighted mean absolute error), \n",
    "- RMSLE (root mean squared logarithmic error)…\n",
    "\n",
    "\n",
    "\n",
    "In regression model, the most commonly known evaluation metrics include:\n",
    "\n",
    "__R-squared (R2)__, which is the proportion of variation in the outcome that is explained by the predictor variables. In multiple regression models, R2 corresponds to the squared correlation between the observed outcome values and the predicted values by the model. The Higher the R-squared, the better the model.\n",
    "\n",
    "__Root Mean Squared Error (RMSE)__\n",
    "\n",
    "- RMSE follows an assumption that error are unbiased and follow a normal distribution. As compared to mean absolute error, RMSE gives higher weightage and punishes large errors.\n",
    "\n",
    "- Because the MSE is squared, its units do not match that of the original output. RMSE is the square root of MSE.\n",
    "- Since the MSE and RMSE both square the residual, they are similarly affected by outliers.\n",
    "- The RMSE is analogous to the standard deviation and is a measure of how large the residuals are spread out.\n",
    "- Generally, RMSE will be higher than or equal to MAE. The lower the RMSE, the better the model.\n",
    "\n",
    "__Residual Standard Error (RSE)__, also known as the model sigma, is a variant of the RMSE adjusted for the number of predictors in the model. The lower the RSE, the better the model. In practice, the difference between RMSE and RSE is very small, particularly for large multivariate data.\n",
    "\n",
    "__Mean Absolute Error (MAE)__, like the RMSE, the MAE measures the prediction error. Mathematically, it is the average absolute difference between observed and predicted outcomes, MAE = mean(abs(observeds - predicteds)). MAE is less sensitive to outliers compared to RMSE.\n",
    "\n",
    "- Average of the difference between the Original Values and the Predicted Values.\n",
    "- Do not gives any idea of the direction of the error i.e. whether we are under predicting the data or over predicting the data.\n",
    "- Smaller the MAE, better is the model.\n",
    "- Robust to outliers\n",
    "- Range (0, + infinity]\n",
    "\n",
    "__Mean Squared Error__\n",
    "\n",
    "- Takes the average of the square of the difference between the original values and the predicted values.\n",
    "- As we take square of the error, the effect of larger errors(sometimes outliers) become more pronounced then smaller error. Model will be penalized more for making predictions that differ greatly from the corresponding actual value.\n",
    "- Before applying MSE, we must eliminate all nulls/infinites from the input.\n",
    "- Not robust to outliers\n",
    "- Range (0, + infinity]\n",
    "\n",
    "__Root Mean Squared Logarithmic Error__\n",
    "\n",
    "- We take the log of the predictions and actual values.\n",
    "- What changes are the variance that we are measuring.\n",
    "- RMSLE is usually used when we don’t want to penalize huge differences in the predicted and the actual values when both predicted and actual values are huge numbers.\n",
    "- If both predicted and actual values are small: RMSE and RMSLE are same.\n",
    "- If either predicted or the actual value is big: RMSE > RMSLE\n",
    "- If both predicted and actual values are big: RMSE > RMSLE (RMSLE becomes almost negligible)\n",
    "\n",
    "The problem with the above metrics, is that they are sensible to the inclusion of additional variables in the model, even if those variables dont have significant contribution in explaining the outcome. Put in other words, including additional variables in the model will always increase the R2 and reduce the RMSE. So, we need a more robust metric to guide the model choice.\n",
    "\n",
    "Concerning R2, there is an adjusted version, called Adjusted R-squared, which adjusts the R2 for having too many variables in the model.\n",
    "\n",
    "Additionally, there are four other important metrics - AIC, AICc, BIC and Mallows Cp - that are commonly used for model evaluation and selection. These are an unbiased estimate of the model prediction error MSE. The lower these metrics, he better the model.\n",
    "\n",
    "AIC stands for (Akaike’s Information Criteria), a metric developped by the Japanese Statistician, Hirotugu Akaike, 1970. The basic idea of AIC is to penalize the inclusion of additional variables to a model. It adds a penalty that increases the error when including additional terms. The lower the AIC, the better the model.\n",
    "AICc is a version of AIC corrected for small sample sizes.\n",
    "BIC (or Bayesian information criteria) is a variant of AIC with a stronger penalty for including additional variables to the model.\n",
    "\n",
    "Mallows Cp: A variant of AIC developed by Colin Mallows.\n",
    "\n",
    "\n",
    "Common metrics in regression:\n",
    "\n",
    "Mean Squared Error Vs Mean Absolute Error RMSE gives a relatively high weight to large errors. The RMSE is most useful when large errors are particularly undesirable.\n",
    "\n",
    "The MAE is a linear score: all the individual differences are weighted equally in the average. MAE is more robust to outliers than MSE.\n",
    "\n",
    "RMSE=1n∑ni=1(yi−y^i)2−−−−−−−−−−−−−−√\n",
    "MAE=1n∑ni=1|yi−y^i|\n",
    "\n",
    "Root Mean Squared Logarithmic Error\n",
    "RMSLE penalizes an under-predicted estimate greater than an over-predicted estimate (opposite to RMSE)\n",
    "\n",
    "RMSLE=1n∑ni=1(log(pi+1)−log(ai+1))2−−−−−−−−−−−−−−−−−−−−−−−−−−−√\n",
    "Where pi is the ith prediction, ai the ith actual response, log(b) the natural logarithm of b.\n",
    "\n",
    "Weighted Mean Absolute Error\n",
    "The weighted average of absolute errors. MAE and RMSE consider that each prediction provides equally precise information about the error variation, i.e. the standard variation of the error term is constant over all the predictions. Examples: recommender systems (differences between past and recent products)\n",
    "WMAE=1∑wi∑ni=1wi|yi−y^i|\n",
    "\n",
    "\n",
    "minimizing the squared error over a set of numbers results in finding its mean, and minimizing the absolute error results in finding its median. This is the reason why MAE is robust to outliers whereas RMSE is not. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q Why are some scores like MSE negative in scikit-learn?__\n",
    "\n",
    "Some model evaluation metrics such as mean squared error (MSE) are negative when calculated in scikit-learn.\n",
    "\n",
    "This is confusing, because error scores like MSE cannot actually be negative, with the smallest value being zero or no error.\n",
    "\n",
    "The scikit-learn library has a unified model scoring system where it assumes that all model scores are maximized. In order this system to work with scores that are minimized, like MSE and other measures of error, the sores that are minimized are inverted by making them negative.\n",
    "\n",
    "This can also be seen in the specification of the metric, e.g. ‘neg‘ is used in the name of the metric ‘neg_mean_squared_error‘.\n",
    "\n",
    "When interpreting the negative error scores, you can ignore the sign and use them directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__MAE vs. MSE__\n",
    "\n",
    "- Being more complex and biased towards higher deviation, RMSE is still the default metric of many models because loss function defined in terms of RMSE is smoothly differentiable whereas Mean Absolute Error requires complicated linear programming to compute the gradient.\n",
    "- If we want a metric just to compare between two models from interpretation point of view, then MAE may be a better choice.\n",
    "- Units of both RMSE & MAE are same as y values which is not true for R Square.\n",
    "- Minimizing the squared error (𝐿2) over a set of numbers results in finding its mean, and minimizing the absolute error (𝐿1) results in finding its median.\n",
    "\n",
    "\n",
    "__Adjusted R² over RMSE__\n",
    "\n",
    "Absolute value of RMSE does not actually tell how good/bad a model is. It can only be used to compare across two models whereas Adjusted R² easily does that. For example, if a model has adjusted R² equal to 0.05 then it is definitely bad.\n",
    "\n",
    "However, if we care only about prediction accuracy then RMSE is best. It is computationally simple, easily differentiable and present as default metric for most of the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi class classification\n",
    "\n",
    "- Multi Class : classify a set of images of fruits into any one of these categories — apples, bananas, and oranges.\n",
    "\n",
    "- Multi label : tagging a blog into one or more topics like technology, religion, politics\n",
    "\n",
    "\n",
    "- MultiClass Classifiers can distinguish between more than two classes.\n",
    "\n",
    "Random Forest Classifiers or Naive Bayes Classifiers are capable of handling multiple classes directly. Others (Support Vector Machine classifiers or Linear classifiers) are strictly binary classifiers.\n",
    "\n",
    "Task: 0 - 9 digits classification\n",
    "One vs All (OvA) Classification Strategy : \n",
    "Train 10 binary classifiers, one for each digit (a 0-detector, a 1-detector, a 2-detector, and so on). When we want to classify an image, we get the decision score from each classifier for that image and we select the class whose classifier outputs the highest score.\n",
    "\n",
    "Example : Almost all classification algorithms.\n",
    "\n",
    "One vs One (OvO) Strategy : \n",
    "Train a binary classifier for every pair of digits: one to distinguish 0s and 1s, another to distinguish 0s and 2s, another for 1s and 2s, and so on. If there are N classes, we need to train N × (N – 1) / 2 classifiers. For the MNIST problem, this means training 45 binary classifiers. When we want to classify an image, we have to run the image through all 45 classifiers and see which class wins the most duels. Main advantage of OvO is that each classifier only needs to be trained on the part of the training set for the two classes that it must distinguish.\n",
    "\n",
    "Example : Support Vector Machines scale poorly with the size of the training set, it is faster to train many classifiers on small training sets than training few classifiers on large training sets.\n",
    "\n",
    "Scikit-Learn detects when we try to use a binary classification algorithm for a multi‐ class classification task, and it automatically runs OvA (except for SVM classifiers for which it uses OvO).For MNIST problem, Under the hood, Scikit-Learn trained 10 binary classifiers, get their decision scores for the image, and selected the class with the highest score.\n",
    "\n",
    "If we want to force ScikitLearn to use OvO or OvA, we can use the OneVsOneClassifier or OneVsRestClassifier classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Metric \n",
    "\n",
    "BLEU (Bilingual Evaluation Understudy)\n",
    "\n",
    "It is mostly used to measure the quality of machine translation with respect to the human translation. It uses a modified form of precision metric.\n",
    "\n",
    "Example: Reference: The cat is sitting on the mat\n",
    "\n",
    "Machine Translation 1: On the mat is a cat\n",
    "\n",
    "Machine Translation 2: There is cat sitting cat\n",
    "\n",
    "Machine Translation 3: The cat is sitting on the tam"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
